{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26a983ec",
   "metadata": {},
   "source": [
    "# Test Plugin Integration\n",
    "\n",
    "> Test the Voxtral VLLM plugin with the transcription plugin system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d4e6a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Import PluginManager from the generic plugin system\n",
    "from cjm_plugin_system.core.manager import PluginManager\n",
    "from cjm_transcription_plugin_system.plugin_interface import TranscriptionPlugin\n",
    "from cjm_transcription_plugin_system.core import AudioData\n",
    "from cjm_transcription_plugin_voxtral_vllm.plugin import VoxtralVLLMPlugin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd72c54",
   "metadata": {},
   "source": [
    "## Test Direct Plugin Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ad0818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plugin: voxtral_vllm v1.0.0\n",
      "Available: True\n",
      "Supported formats: wav, mp3, flac, m4a, ogg, webm, mp4, avi, mov\n",
      "Supports streaming: True\n"
     ]
    }
   ],
   "source": [
    "# Create plugin directly\n",
    "plugin = VoxtralVLLMPlugin()\n",
    "\n",
    "# Check basic properties\n",
    "print(f\"Plugin: {plugin.name} v{plugin.version}\")\n",
    "print(f\"Available: {plugin.is_available()}\")\n",
    "print(f\"Supported formats: {', '.join(plugin.supported_formats)}\")\n",
    "print(f\"Supports streaming: {plugin.supports_streaming()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b6b2d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration options:\n",
      "- Required: ['model_id']\n",
      "- Total properties: 14\n",
      "\n",
      "Available models:\n",
      "  - mistralai/Voxtral-Mini-3B-2507\n",
      "  - mistralai/Voxtral-Small-24B-2507\n",
      "\n",
      "Server modes:\n",
      "  - managed: 'managed': plugin manages server lifecycle, 'external': connect to existing server\n",
      "  - external\n"
     ]
    }
   ],
   "source": [
    "# Get configuration schema\n",
    "schema = plugin.get_config_schema()\n",
    "print(\"Configuration options:\")\n",
    "print(f\"- Required: {schema.get('required', [])}\")\n",
    "print(f\"- Total properties: {len(schema['properties'])}\")\n",
    "print(\"\\nAvailable models:\")\n",
    "for model in schema['properties']['model_id']['enum']:\n",
    "    print(f\"  - {model}\")\n",
    "print(\"\\nServer modes:\")\n",
    "for mode in schema['properties']['server_mode']['enum']:\n",
    "    print(f\"  - {mode}: {schema['properties']['server_mode']['description']}\" if mode == \"managed\" else f\"  - {mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b06f1",
   "metadata": {},
   "source": [
    "## Test with Plugin Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73264a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Create plugin manager\n",
    "manager = PluginManager(plugin_interface=TranscriptionPlugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "082df72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Initialized Voxtral VLLM plugin with model 'mistralai/Voxtral-Mini-3B-2507' in managed mode\n",
      "cjm_plugin_system.core.manager.PluginManager - INFO - Loaded plugin from module: voxtral_vllm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plugin loaded: True\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Load plugin from module directly (for development)\n",
    "# This works even without the package being installed\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path to import the plugin module\n",
    "parent_dir = Path.cwd().parent\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "# Import and register the plugin\n",
    "from cjm_transcription_plugin_voxtral_vllm.plugin import VoxtralVLLMPlugin\n",
    "\n",
    "# Create a temporary module file for the plugin manager to load\n",
    "temp_plugin_file = Path(\"temp_voxtral_vllm_plugin.py\")\n",
    "with open(temp_plugin_file, \"w\") as f:\n",
    "    f.write(\"from cjm_transcription_plugin_voxtral_vllm.plugin import VoxtralVLLMPlugin\\n\")\n",
    "\n",
    "# Load the plugin with managed server mode for testing\n",
    "success = manager.load_plugin_from_module(\n",
    "    str(temp_plugin_file),\n",
    "    config={\n",
    "        \"model_id\": \"mistralai/Voxtral-Mini-3B-2507\",\n",
    "        \"server_mode\": \"managed\",\n",
    "        \"server_url\": \"http://localhost:8000\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Plugin loaded: {success}\")\n",
    "\n",
    "# Clean up temp file\n",
    "temp_plugin_file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b67bea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded plugins:\n",
      "  - voxtral_vllm v1.0.0 (enabled: True)\n"
     ]
    }
   ],
   "source": [
    "# List loaded plugins\n",
    "print(\"Loaded plugins:\")\n",
    "for meta in manager.list_plugins():\n",
    "    print(f\"  - {meta.name} v{meta.version} (enabled: {meta.enabled})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29885ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Voxtral VLLM configuration:\n",
      "{\n",
      "  \"model_id\": \"mistralai/Voxtral-Mini-3B-2507\",\n",
      "  \"server_mode\": \"managed\",\n",
      "  \"server_url\": \"http://localhost:8000\",\n",
      "  \"language\": \"en\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get plugin configuration\n",
    "current_config = manager.get_plugin_config(\"voxtral_vllm\")\n",
    "print(\"Current Voxtral VLLM configuration:\")\n",
    "config_subset = {k: current_config[k] for k in [\"model_id\", \"server_mode\", \"server_url\", \"language\"] if k in current_config}\n",
    "print(json.dumps(config_subset, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd87611",
   "metadata": {},
   "source": [
    "## Test Configuration Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b17f0436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: switching to Small model\n",
      "  Config: {'model_id': 'mistralai/Voxtral-Small-24B-2507'}\n",
      "  Valid: True\n",
      "\n",
      "Invalid: bad model name\n",
      "  Config: {'model_id': 'invalid_model'}\n",
      "  Valid: False\n",
      "  Error: 'invalid_model' is not one of ['mistralai/Voxtral-Mini-3B-2507', 'mistralai/Voxtral-Small-24B-2507']...\n",
      "\n",
      "Valid: adjusting temperature\n",
      "  Config: {'temperature': 0.5}\n",
      "  Valid: False\n",
      "  Error: 'model_id' is a required property\n",
      "\n",
      "Failed validating 'required' in schema:\n",
      "    {'$schema': 'http://j...\n",
      "\n",
      "Valid: changing server port\n",
      "  Config: {'server_port': 9000}\n",
      "  Valid: False\n",
      "  Error: 'model_id' is a required property\n",
      "\n",
      "Failed validating 'required' in schema:\n",
      "    {'$schema': 'http://j...\n",
      "\n",
      "Invalid: GPU memory out of range\n",
      "  Config: {'gpu_memory_utilization': 1.5}\n",
      "  Valid: False\n",
      "  Error: 'model_id' is a required property\n",
      "\n",
      "Failed validating 'required' in schema:\n",
      "    {'$schema': 'http://j...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate different configurations\n",
    "test_configs = [\n",
    "    ({\"model_id\": \"mistralai/Voxtral-Small-24B-2507\"}, \"Valid: switching to Small model\"),\n",
    "    ({\"model_id\": \"invalid_model\"}, \"Invalid: bad model name\"),\n",
    "    ({\"temperature\": 0.5}, \"Valid: adjusting temperature\"),\n",
    "    ({\"server_port\": 9000}, \"Valid: changing server port\"),\n",
    "    ({\"gpu_memory_utilization\": 1.5}, \"Invalid: GPU memory out of range\"),\n",
    "]\n",
    "\n",
    "for config, description in test_configs:\n",
    "    is_valid, error = manager.validate_plugin_config(\"voxtral_vllm\", config)\n",
    "    print(f\"{description}\")\n",
    "    print(f\"  Config: {config}\")\n",
    "    print(f\"  Valid: {is_valid}\")\n",
    "    if error:\n",
    "        print(f\"  Error: {error[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc1941bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Cleaning up Voxtral VLLM plugin\n",
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Cleanup completed successfully\n",
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Initialized Voxtral VLLM plugin with model 'mistralai/Voxtral-Mini-3B-2507' in managed mode\n",
      "cjm_plugin_system.core.manager.PluginManager - INFO - Updated configuration for plugin: voxtral_vllm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration updated: True\n",
      "\n",
      "Updated configuration:\n",
      "  temperature: 0.0\n",
      "  language: en\n",
      "  streaming: True\n"
     ]
    }
   ],
   "source": [
    "# Update configuration\n",
    "new_config = {\n",
    "    \"temperature\": 0.0,\n",
    "    \"language\": \"en\",\n",
    "    \"streaming\": True,\n",
    "}\n",
    "\n",
    "success = manager.update_plugin_config(\"voxtral_vllm\", new_config, merge=True)\n",
    "print(f\"Configuration updated: {success}\")\n",
    "\n",
    "if success:\n",
    "    updated_config = manager.get_plugin_config(\"voxtral_vllm\")\n",
    "    print(\"\\nUpdated configuration:\")\n",
    "    for key in new_config:\n",
    "        print(f\"  {key}: {updated_config[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d4f37c",
   "metadata": {},
   "source": [
    "## Test with Managed Server Mode\n",
    "\n",
    "This section demonstrates using the plugin with a managed vLLM server that the plugin starts and stops automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bafded6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Initialized Voxtral VLLM plugin with model 'mistralai/Voxtral-Mini-3B-2507' in managed mode\n",
      "cjm_plugin_system.core.manager.PluginManager - INFO - Loaded plugin from module: voxtral_vllm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Managed plugin loaded: True\n"
     ]
    }
   ],
   "source": [
    "# Create a new manager for managed server testing\n",
    "manager_managed = PluginManager(plugin_interface=TranscriptionPlugin)\n",
    "\n",
    "# Create temp module file\n",
    "temp_plugin_file = Path(\"temp_voxtral_vllm_managed.py\")\n",
    "with open(temp_plugin_file, \"w\") as f:\n",
    "    f.write(\"from cjm_transcription_plugin_voxtral_vllm.plugin import VoxtralVLLMPlugin\\n\")\n",
    "\n",
    "# Load plugin with managed server mode\n",
    "success = manager_managed.load_plugin_from_module(\n",
    "    str(temp_plugin_file),\n",
    "    config={\n",
    "        \"model_id\": \"mistralai/Voxtral-Mini-3B-2507\",\n",
    "        \"server_mode\": \"managed\",\n",
    "        \"server_port\": 8001,  # Use different port to avoid conflicts\n",
    "        \"auto_start_server\": True,\n",
    "        \"gpu_memory_utilization\": 0.85,\n",
    "        \"capture_server_logs\": True,  # Enable log capture to see server startup progress\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Managed plugin loaded: {success}\")\n",
    "temp_plugin_file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "824a7130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.config import get_config\n",
    "from pathlib import Path\n",
    "\n",
    "config = get_config()\n",
    "project_dir = config.config_path\n",
    "test_dir = project_dir/\"./test_files/\"\n",
    "audio_path = test_dir/\"short_test_audio.mp3\"\n",
    "assert audio_path.exists(), f\"Test audio file not found at {audio_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae02d83b",
   "metadata": {},
   "source": [
    "## Test with Managed Server (Auto-start)\n",
    "\n",
    "This test will automatically start a vLLM server if you have vLLM installed and a GPU available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c749196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Starting vLLM server...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with managed server (this may take a minute to start the server)...\n",
      "\n",
      "\n",
      "SERVER IS NOT RUNNING\n",
      "\n",
      "\n",
      "Starting vLLM server with model mistralai/Voxtral-Mini-3B-2507...\n",
      "\u001b[94m10-21 17:42:45 INFO 10-21 17:42:45 [__init__.py:241] Automatically detected platform cuda.\u001b[0m\n",
      "\n",
      "  🔍 Detecting platform...\n",
      "\u001b[94m10-21 17:42:46 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:42:46 [api_server.py:1805] vLLM API server version 0.10.1.1\u001b[0m\n",
      "\u001b[94m10-21 17:42:46 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:42:46 [utils.py:326] non-default args: {'host': '0.0.0.0', 'port': 8001, 'model': 'mistralai/Voxtral-Mini-3B-2507', 'tokenizer_mode': 'mistral', 'max_model_len': 32768, 'config_format': 'mistral', 'load_format': 'mistral', 'gpu_memory_utilization': 0.85}\u001b[0m\n",
      "\u001b[94m10-21 17:42:49 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:42:49 [__init__.py:711] Resolved architecture: VoxtralForConditionalGeneration\u001b[0m\n",
      "\u001b[94m10-21 17:42:50 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:42:50 [__init__.py:2816] Downcasting torch.float32 to torch.bfloat16.\u001b[0m\n",
      "\u001b[94m10-21 17:42:50 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:42:50 [__init__.py:1750] Using max model len 32768\u001b[0m\n",
      "\u001b[94m10-21 17:42:51 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:42:51 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\u001b[0m\n",
      "\u001b[94m10-21 17:42:54 INFO 10-21 17:42:54 [__init__.py:241] Automatically detected platform cuda.\u001b[0m\n",
      "\u001b[94m10-21 17:42:55 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m INFO 10-21 17:42:55 [core.py:636] Waiting for init message from front-end.\u001b[0m\n",
      "\u001b[94m10-21 17:42:55 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m INFO 10-21 17:42:55 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='mistralai/Voxtral-Mini-3B-2507', speculative_config=None, tokenizer='mistralai/Voxtral-Mini-3B-2507', skip_tokenizer_init=False, tokenizer_mode=mistral, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=mistral, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Voxtral-Mini-3B-2507, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\u001b[0m\n",
      "\u001b[94m10-21 17:42:56 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m INFO 10-21 17:42:56 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\u001b[0m\n",
      "\u001b[93m10-21 17:42:57 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m WARNING 10-21 17:42:57 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\u001b[0m\n",
      "\u001b[94m10-21 17:42:57 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m INFO 10-21 17:42:57 [gpu_model_runner.py:1953] Starting to load model mistralai/Voxtral-Mini-3B-2507...\u001b[0m\n",
      "\u001b[94m10-21 17:42:57 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m INFO 10-21 17:42:57 [gpu_model_runner.py:1985] Loading model from scratch...\u001b[0m\n",
      "\u001b[94m10-21 17:42:57 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m INFO 10-21 17:42:57 [cuda.py:328] Using Flash Attention backend on V1 engine.\u001b[0m\n",
      "\u001b[94m10-21 17:42:58 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m INFO 10-21 17:42:58 [weight_utils.py:296] Using model weights format ['consolidated*.safetensors', '*.pt']\u001b[0m\n",
      "\u001b[94m10-21 17:42:58 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m INFO 10-21 17:42:58 [weight_utils.py:349] No consolidated.safetensors.index.json found in remote.\u001b[0m\n",
      "\n",
      "  📦 Loading model weights...\n",
      "\u001b[94m10-21 17:43:06 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m INFO 10-21 17:43:06 [default_loader.py:262] Loading weights took 8.26 seconds\u001b[0m\n",
      "\n",
      "  ✅ Model weights loaded\n",
      "\u001b[94m10-21 17:43:07 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m INFO 10-21 17:43:07 [gpu_model_runner.py:2007] Model loading took 8.7223 GiB and 8.823797 seconds\u001b[0m\n",
      "\u001b[94m10-21 17:43:07 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m INFO 10-21 17:43:07 [gpu_model_runner.py:2591] Encoder cache will be initialized with a budget of 32768 tokens, and profiled with 1 audio items of the maximum feature size.\u001b[0m\n",
      "\u001b[93m10-21 17:43:07 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m WARNING 10-21 17:43:07 [registry.py:183] VoxtralProcessorAdapter did not return `BatchFeature`. Make sure to match the behaviour of `ProcessorMixin` when implementing custom processors.\u001b[0m\n",
      "\u001b[94m10-21 17:43:12 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m INFO 10-21 17:43:12 [backends.py:548] Using cache directory: /home/innom-dt/.cache/vllm/torch_compile_cache/2395c5741c/rank_0_0/backbone for vLLM's torch.compile\u001b[0m\n",
      "\u001b[94m10-21 17:43:12 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m INFO 10-21 17:43:12 [backends.py:559] Dynamo bytecode transform time: 2.78 s\u001b[0m\n",
      "\u001b[94m10-21 17:43:15 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m INFO 10-21 17:43:15 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.373 s\u001b[0m\n",
      "\u001b[94m10-21 17:43:15 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m INFO 10-21 17:43:15 [monitor.py:34] torch.compile takes 2.78 s in total\u001b[0m\n",
      "\u001b[94m10-21 17:43:16 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m INFO 10-21 17:43:16 [gpu_worker.py:276] Available KV cache memory: 6.86 GiB\u001b[0m\n",
      "\u001b[94m10-21 17:43:16 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m INFO 10-21 17:43:16 [kv_cache_utils.py:849] GPU KV cache size: 59,904 tokens\u001b[0m\n",
      "\u001b[94m10-21 17:43:16 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m INFO 10-21 17:43:16 [kv_cache_utils.py:853] Maximum concurrency for 32,768 tokens per request: 1.83x\u001b[0m\n",
      "\u001b[94m10-21 17:43:18 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m INFO 10-21 17:43:18 [gpu_model_runner.py:2708] Graph capturing finished in 2 secs, took 0.46 GiB\u001b[0m\n",
      "\u001b[94m10-21 17:43:18 \u001b[1;36m(EngineCore_0 pid=722316)\u001b[0;0m INFO 10-21 17:43:18 [core.py:214] init engine (profile, create kv cache, warmup model) took 10.85 seconds\u001b[0m\n",
      "\n",
      "  📊 Capturing CUDA graphs...\n",
      "\u001b[94m10-21 17:43:18 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:18 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 3744\u001b[0m\n",
      "\u001b[94m10-21 17:43:18 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:18 [api_server.py:1611] Supported_tasks: ['generate', 'transcription']\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [api_server.py:1880] Starting vLLM API server 0 on http://0.0.0.0:8001\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:36] Available routes are:\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /openapi.json, Methods: HEAD, GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /docs, Methods: HEAD, GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /docs/oauth2-redirect, Methods: HEAD, GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /redoc, Methods: HEAD, GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /health, Methods: GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /load, Methods: GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /ping, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /ping, Methods: GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /tokenize, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /detokenize, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /v1/models, Methods: GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /version, Methods: GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /v1/responses, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /v1/chat/completions, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /v1/completions, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /v1/embeddings, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /pooling, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /classify, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /score, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /v1/score, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /v1/audio/translations, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /rerank, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /v1/rerank, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /v2/rerank, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /scale_elastic_ep, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /invocations, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO 10-21 17:43:19 [launcher.py:44] Route: /metrics, Methods: GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO:     Started server process [721874]\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO:     Waiting for application startup.\u001b[0m\n",
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO:     Application startup complete.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Processing audio with Voxtral mistralai/Voxtral-Mini-3B-2507 via vLLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m10-21 17:43:19 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO:     127.0.0.1:47126 - \"GET /health HTTP/1.1\" 200 OK\u001b[0m\n",
      "✅ vLLM server is ready at http://localhost:8001\n",
      "\n",
      "\u001b[93m10-21 17:43:21 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m WARNING 10-21 17:43:21 [registry.py:183] VoxtralProcessorAdapter did not return `BatchFeature`. Make sure to match the behaviour of `ProcessorMixin` when implementing custom processors.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "httpx - INFO - HTTP Request: POST http://localhost:8001/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Transcription completed: 43 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m10-21 17:43:21 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO:     127.0.0.1:49666 - \"POST /v1/audio/transcriptions HTTP/1.1\" 200 OK\u001b[0m\n",
      "\n",
      "Transcription result:\n",
      "  Text: November the 10th, Wednesday, 9 p.m. I'm standing in a dark alley. After waiting several hours, the time has come. A woman with long, dark hair approaches. I have to act and fast before she realizes what has happened. I must find out.\n",
      "  Server mode: managed\n",
      "\u001b[94m10-21 17:43:21 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO:     127.0.0.1:49670 - \"GET /health HTTP/1.1\" 200 OK\u001b[0m\n",
      "\u001b[94m10-21 17:43:22 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO:     127.0.0.1:49666 - \"POST /v1/audio/transcriptions HTTP/1.1\" 200 OK\u001b[0m\n",
      "\u001b[94m10-21 17:43:22 \u001b[1;36m(APIServer pid=721874)\u001b[0;0m INFO:     127.0.0.1:49686 - \"GET /health HTTP/1.1\" 200 OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Test transcription with managed server (will auto-start server)\n",
    "try:\n",
    "    print(\"Testing with managed server (this may take a minute to start the server)...\")\n",
    "    result = manager_managed.execute_plugin(\"voxtral_vllm\", audio_path)\n",
    "    print(\"\\nTranscription result:\")\n",
    "    print(f\"  Text: {result.text}\")\n",
    "    print(f\"  Server mode: {result.metadata.get('server_mode')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"This requires vLLM to be installed and a GPU to be available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450d193e-3b0b-4ca2-853a-00dc1a05b113",
   "metadata": {},
   "source": [
    "## Test Streaming Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc1115e9-4a8b-4733-89b5-fc0df752a923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cjm_plugin_system.core.manager.PluginManager - INFO - Using streaming mode for plugin voxtral_vllm\n",
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Streaming transcription with Voxtral mistralai/Voxtral-Mini-3B-2507 via vLLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing streaming transcription:\n",
      "Streaming output: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "httpx - INFO - HTTP Request: POST http://localhost:8001/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "November the 10th, Wednesday, 9 p.m. I'm standing in a dark alley. After waiting several hours, the time has come. A woman with long, dark hair approaches. I have to act and fast before she realizes what has happened. I must find out.\n",
      "\n",
      "Streaming completed!\n"
     ]
    }
   ],
   "source": [
    "# Test streaming transcription\n",
    "try:\n",
    "    print(\"Testing streaming transcription:\")\n",
    "    print(\"Streaming output: \", end=\"\")\n",
    "    for chunk in manager_managed.execute_plugin_stream(\"voxtral_vllm\", audio_path):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print(\"\\n\\nStreaming completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError: {e}\")\n",
    "    print(\"Make sure a vLLM server is running on port 8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0884e71",
   "metadata": {},
   "source": [
    "## Test Plugin Lifecycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21bbf13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing plugin lifecycle:\n",
      "Plugin disabled: True\n",
      "Expected error: Plugin voxtral_vllm is disabled\n",
      "Plugin re-enabled: True\n"
     ]
    }
   ],
   "source": [
    "# Test disabling and enabling\n",
    "print(\"Testing plugin lifecycle:\")\n",
    "\n",
    "# Disable plugin\n",
    "manager.disable_plugin(\"voxtral_vllm\")\n",
    "print(f\"Plugin disabled: {not manager.plugins['voxtral_vllm'].enabled}\")\n",
    "\n",
    "# Try to execute while disabled (should fail)\n",
    "try:\n",
    "    manager.execute_plugin(\"voxtral_vllm\", audio_path)\n",
    "except ValueError as e:\n",
    "    print(f\"Expected error: {e}\")\n",
    "\n",
    "# Re-enable plugin\n",
    "manager.enable_plugin(\"voxtral_vllm\")\n",
    "print(f\"Plugin re-enabled: {manager.plugins['voxtral_vllm'].enabled}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0d16a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Cleaning up Voxtral VLLM plugin\n",
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Cleanup completed successfully\n",
      "cjm_plugin_system.core.manager.PluginManager - INFO - Unloaded plugin: voxtral_vllm\n",
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Cleaning up Voxtral VLLM plugin\n",
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Stopping managed vLLM server\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning up...\n",
      "Managed server plugin unloaded. Plugins remaining: 0\n",
      "Stopping vLLM server...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Cleanup completed successfully\n",
      "cjm_plugin_system.core.manager.PluginManager - INFO - Unloaded plugin: voxtral_vllm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server stopped\n",
      "Managed server plugin unloaded. Plugins remaining: 0\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "print(\"\\nCleaning up...\")\n",
    "\n",
    "# Unload managed server plugin\n",
    "manager.unload_plugin(\"voxtral_vllm\")\n",
    "print(f\"Managed server plugin unloaded. Plugins remaining: {len(manager.list_plugins())}\")\n",
    "\n",
    "# Unload managed server plugin (will stop the server if running)\n",
    "if 'voxtral_vllm' in manager_managed.plugins:\n",
    "    manager_managed.unload_plugin(\"voxtral_vllm\")\n",
    "    print(f\"Managed server plugin unloaded. Plugins remaining: {len(manager_managed.list_plugins())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d7515b",
   "metadata": {},
   "source": [
    "## Test Entry Point Discovery (After Installation)\n",
    "\n",
    "This will work after the package is installed with `pip install -e .` or `pip install cjm-transcription-plugin-voxtral-vllm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "165d0df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cjm_plugin_system.core.manager.PluginManager - INFO - Discovered plugin: voxtral_vllm v0.0.2 from package cjm-transcription-plugin-voxtral-vllm\n",
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Initialized Voxtral VLLM plugin with model 'mistralai/Voxtral-Mini-3B-2507' in managed mode\n",
      "cjm_plugin_system.core.manager.PluginManager - INFO - Loaded plugin: voxtral_vllm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing entry point discovery:\n",
      "\n",
      "Discovered 1 plugin(s) via entry points:\n",
      "  - voxtral_vllm v0.0.2 from cjm-transcription-plugin-voxtral-vllm\n",
      "\n",
      "Loaded voxtral_vllm: True\n"
     ]
    }
   ],
   "source": [
    "# This will only work after the package is installed\n",
    "print(\"Testing entry point discovery:\")\n",
    "manager2 = PluginManager(plugin_interface=TranscriptionPlugin)\n",
    "\n",
    "# Discover plugins via entry points\n",
    "discovered = manager2.discover_plugins()\n",
    "print(f\"\\nDiscovered {len(discovered)} plugin(s) via entry points:\")\n",
    "for plugin_meta in discovered:\n",
    "    print(f\"  - {plugin_meta.name} v{plugin_meta.version} from {plugin_meta.package_name}\")\n",
    "\n",
    "# Load discovered plugins\n",
    "for plugin_meta in discovered:\n",
    "    if plugin_meta.name == \"voxtral_vllm\":\n",
    "        success = manager2.load_plugin(\n",
    "            plugin_meta,\n",
    "            config={\n",
    "                \"model_id\": \"mistralai/Voxtral-Mini-3B-2507\",\n",
    "                \"server_mode\": \"managed\",\n",
    "                \"server_url\": \"http://localhost:8000\"\n",
    "            }\n",
    "        )\n",
    "        print(f\"\\nLoaded {plugin_meta.name}: {success}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17f78fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Starting vLLM server...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing: /mnt/SN850X_8TB_EXT4/Projects/GitHub/cj-mills/cjm-transcription-plugin-voxtral-vllm/test_files/short_test_audio.mp3\n",
      "\n",
      "\n",
      "SERVER IS NOT RUNNING\n",
      "\n",
      "\n",
      "Starting vLLM server with model mistralai/Voxtral-Mini-3B-2507...\n",
      "\u001b[94m10-21 17:43:26 INFO 10-21 17:43:26 [__init__.py:241] Automatically detected platform cuda.\u001b[0m\n",
      "\n",
      "  🔍 Detecting platform...\n",
      "\u001b[94m10-21 17:43:27 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:27 [api_server.py:1805] vLLM API server version 0.10.1.1\u001b[0m\n",
      "\u001b[94m10-21 17:43:27 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:27 [utils.py:326] non-default args: {'host': '0.0.0.0', 'model': 'mistralai/Voxtral-Mini-3B-2507', 'tokenizer_mode': 'mistral', 'max_model_len': 32768, 'config_format': 'mistral', 'load_format': 'mistral', 'gpu_memory_utilization': 0.85}\u001b[0m\n",
      "\u001b[94m10-21 17:43:30 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:30 [__init__.py:711] Resolved architecture: VoxtralForConditionalGeneration\u001b[0m\n",
      "\u001b[94m10-21 17:43:31 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:31 [__init__.py:2816] Downcasting torch.float32 to torch.bfloat16.\u001b[0m\n",
      "\u001b[94m10-21 17:43:31 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:31 [__init__.py:1750] Using max model len 32768\u001b[0m\n",
      "\u001b[94m10-21 17:43:32 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:32 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\u001b[0m\n",
      "\u001b[94m10-21 17:43:36 INFO 10-21 17:43:36 [__init__.py:241] Automatically detected platform cuda.\u001b[0m\n",
      "\u001b[94m10-21 17:43:36 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m INFO 10-21 17:43:36 [core.py:636] Waiting for init message from front-end.\u001b[0m\n",
      "\u001b[94m10-21 17:43:36 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m INFO 10-21 17:43:36 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='mistralai/Voxtral-Mini-3B-2507', speculative_config=None, tokenizer='mistralai/Voxtral-Mini-3B-2507', skip_tokenizer_init=False, tokenizer_mode=mistral, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=mistral, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Voxtral-Mini-3B-2507, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\u001b[0m\n",
      "\u001b[94m10-21 17:43:37 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m INFO 10-21 17:43:37 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\u001b[0m\n",
      "\u001b[93m10-21 17:43:38 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m WARNING 10-21 17:43:38 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\u001b[0m\n",
      "\u001b[94m10-21 17:43:38 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m INFO 10-21 17:43:38 [gpu_model_runner.py:1953] Starting to load model mistralai/Voxtral-Mini-3B-2507...\u001b[0m\n",
      "\u001b[94m10-21 17:43:39 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m INFO 10-21 17:43:39 [gpu_model_runner.py:1985] Loading model from scratch...\u001b[0m\n",
      "\u001b[94m10-21 17:43:39 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m INFO 10-21 17:43:39 [cuda.py:328] Using Flash Attention backend on V1 engine.\u001b[0m\n",
      "\u001b[94m10-21 17:43:39 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m INFO 10-21 17:43:39 [weight_utils.py:296] Using model weights format ['consolidated*.safetensors', '*.pt']\u001b[0m\n",
      "\u001b[94m10-21 17:43:39 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m INFO 10-21 17:43:39 [weight_utils.py:349] No consolidated.safetensors.index.json found in remote.\u001b[0m\n",
      "\n",
      "  📦 Loading model weights...\n",
      "\u001b[94m10-21 17:43:40 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m INFO 10-21 17:43:40 [default_loader.py:262] Loading weights took 0.99 seconds\u001b[0m\n",
      "\u001b[94m10-21 17:43:41 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m INFO 10-21 17:43:41 [gpu_model_runner.py:2007] Model loading took 8.7223 GiB and 1.516396 seconds\u001b[0m\n",
      "\u001b[94m10-21 17:43:41 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m INFO 10-21 17:43:41 [gpu_model_runner.py:2591] Encoder cache will be initialized with a budget of 32768 tokens, and profiled with 1 audio items of the maximum feature size.\u001b[0m\n",
      "\u001b[93m10-21 17:43:41 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m WARNING 10-21 17:43:41 [registry.py:183] VoxtralProcessorAdapter did not return `BatchFeature`. Make sure to match the behaviour of `ProcessorMixin` when implementing custom processors.\u001b[0m\n",
      "\n",
      "  ✅ Model weights loaded\n",
      "\u001b[94m10-21 17:43:45 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m INFO 10-21 17:43:45 [backends.py:548] Using cache directory: /home/innom-dt/.cache/vllm/torch_compile_cache/2395c5741c/rank_0_0/backbone for vLLM's torch.compile\u001b[0m\n",
      "\u001b[94m10-21 17:43:45 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m INFO 10-21 17:43:45 [backends.py:559] Dynamo bytecode transform time: 2.40 s\u001b[0m\n",
      "\u001b[94m10-21 17:43:48 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m INFO 10-21 17:43:48 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.202 s\u001b[0m\n",
      "\u001b[94m10-21 17:43:48 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m INFO 10-21 17:43:48 [monitor.py:34] torch.compile takes 2.40 s in total\u001b[0m\n",
      "\u001b[94m10-21 17:43:49 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m INFO 10-21 17:43:49 [gpu_worker.py:276] Available KV cache memory: 7.70 GiB\u001b[0m\n",
      "\u001b[94m10-21 17:43:49 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m INFO 10-21 17:43:49 [kv_cache_utils.py:849] GPU KV cache size: 67,280 tokens\u001b[0m\n",
      "\u001b[94m10-21 17:43:49 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m INFO 10-21 17:43:49 [kv_cache_utils.py:853] Maximum concurrency for 32,768 tokens per request: 2.05x\u001b[0m\n",
      "\u001b[94m10-21 17:43:51 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m INFO 10-21 17:43:51 [gpu_model_runner.py:2708] Graph capturing finished in 2 secs, took 0.46 GiB\u001b[0m\n",
      "\u001b[94m10-21 17:43:51 \u001b[1;36m(EngineCore_0 pid=724147)\u001b[0;0m INFO 10-21 17:43:51 [core.py:214] init engine (profile, create kv cache, warmup model) took 10.06 seconds\u001b[0m\n",
      "\n",
      "  📊 Capturing CUDA graphs...\n",
      "\u001b[94m10-21 17:43:51 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:51 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 4205\u001b[0m\n",
      "\u001b[94m10-21 17:43:51 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:51 [api_server.py:1611] Supported_tasks: ['generate', 'transcription']\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [api_server.py:1880] Starting vLLM API server 0 on http://0.0.0.0:8000\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:36] Available routes are:\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /openapi.json, Methods: HEAD, GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /docs, Methods: HEAD, GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /docs/oauth2-redirect, Methods: HEAD, GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /redoc, Methods: HEAD, GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /health, Methods: GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /load, Methods: GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /ping, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /ping, Methods: GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /tokenize, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /detokenize, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /v1/models, Methods: GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /version, Methods: GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /v1/responses, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /v1/chat/completions, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /v1/completions, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /v1/embeddings, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /pooling, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /classify, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /score, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /v1/score, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /v1/audio/translations, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /rerank, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /v1/rerank, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /v2/rerank, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /scale_elastic_ep, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /invocations, Methods: POST\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO 10-21 17:43:52 [launcher.py:44] Route: /metrics, Methods: GET\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO:     Started server process [723725]\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO:     Waiting for application startup.\u001b[0m\n",
      "\u001b[94m10-21 17:43:52 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO:     Application startup complete.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Processing audio with Voxtral mistralai/Voxtral-Mini-3B-2507 via vLLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m10-21 17:43:53 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO:     127.0.0.1:33700 - \"GET /health HTTP/1.1\" 200 OK\u001b[0m\n",
      "✅ vLLM server is ready at http://localhost:8000\n",
      "\n",
      "\u001b[93m10-21 17:43:53 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m WARNING 10-21 17:43:53 [registry.py:183] VoxtralProcessorAdapter did not return `BatchFeature`. Make sure to match the behaviour of `ProcessorMixin` when implementing custom processors.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "httpx - INFO - HTTP Request: POST http://localhost:8000/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Transcription completed: 43 words\n",
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Cleaning up Voxtral VLLM plugin\n",
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Stopping managed vLLM server\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m10-21 17:43:54 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO:     127.0.0.1:33714 - \"POST /v1/audio/transcriptions HTTP/1.1\" 200 OK\u001b[0m\n",
      "Transcription result:\n",
      "  Text: November the 10th, Wednesday, 9 p.m. I'm standing in a dark alley. After waiting several hours, the time has come. A woman with long, dark hair approaches. I have to act and fast before she realizes what has happened. I must find out.\n",
      "  Metadata: {'model': 'mistralai/Voxtral-Mini-3B-2507', 'language': 'en', 'server_mode': 'managed', 'temperature': 0.0}\n",
      "\u001b[94m10-21 17:43:54 \u001b[1;36m(APIServer pid=723725)\u001b[0;0m INFO:     127.0.0.1:33730 - \"GET /health HTTP/1.1\" 200 OK\u001b[0m\n",
      "Stopping vLLM server...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cjm_transcription_plugin_voxtral_vllm.plugin.VoxtralVLLMPlugin - INFO - Cleanup completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server stopped\n"
     ]
    }
   ],
   "source": [
    "# Test transcription with discovered plugin\n",
    "if len(discovered) > 0 and \"voxtral_vllm\" in [p.name for p in discovered]:\n",
    "    try:\n",
    "        print(f\"Transcribing: {audio_path}\")\n",
    "        result = manager2.execute_plugin(\"voxtral_vllm\", audio_path)\n",
    "        print(\"Transcription result:\")\n",
    "        print(f\"  Text: {result.text}\")\n",
    "        print(f\"  Metadata: {result.metadata}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Make sure a vLLM server is running on port 8000\")\n",
    "    \n",
    "    # Clean up\n",
    "    manager2.get_plugin('voxtral_vllm').cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
