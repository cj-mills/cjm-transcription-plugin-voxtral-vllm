{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb55d2c",
   "metadata": {},
   "source": [
    "# cjm-transcription-plugin-voxtral-vllm\n",
    "\n",
    "> Mistral Voxtral plugin for the cjm-transcription-plugin-system library - provides local speech-to-text transcription through vLLM with configurable model selection and parameter control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fdb3d6",
   "metadata": {},
   "source": [
    "## Install\n",
    "\n",
    "```bash\n",
    "pip install cjm_transcription_plugin_voxtral_vllm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f0c328",
   "metadata": {},
   "source": [
    "## Project Structure\n",
    "\n",
    "```\n",
    "nbs/\n",
    "├── meta.ipynb   # Metadata introspection for the Voxtral VLLM plugin used by cjm-ctl to generate the registration manifest.\n",
    "└── plugin.ipynb # Plugin implementation for Mistral Voxtral transcription through vLLM server\n",
    "```\n",
    "\n",
    "Total: 2 notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dc2ea8",
   "metadata": {},
   "source": [
    "## Module Dependencies\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    meta[meta<br/>Metadata]\n",
    "    plugin[plugin<br/>Voxtral VLLM Plugin]\n",
    "\n",
    "    plugin --> meta\n",
    "```\n",
    "\n",
    "*1 cross-module dependencies detected*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ef6ccf",
   "metadata": {},
   "source": [
    "## CLI Reference\n",
    "\n",
    "No CLI commands found in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecf4794",
   "metadata": {},
   "source": [
    "## Module Overview\n",
    "\n",
    "Detailed documentation for each module in the project:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc661023",
   "metadata": {},
   "source": [
    "### Metadata (`meta.ipynb`)\n",
    "> Metadata introspection for the Voxtral VLLM plugin used by cjm-ctl to generate the registration manifest.\n",
    "\n",
    "#### Import\n",
    "\n",
    "```python\n",
    "from cjm_transcription_plugin_voxtral_vllm.meta import (\n",
    "    get_plugin_metadata\n",
    ")\n",
    "```\n",
    "\n",
    "#### Functions\n",
    "\n",
    "```python\n",
    "def get_plugin_metadata() -> Dict[str, Any]: # Plugin metadata for manifest generation\n",
    "    \"\"\"Return metadata required to register this plugin with the PluginManager.\"\"\"\n",
    "    # Fallback base path (current behavior for backward compatibility)\n",
    "    base_path = os.path.dirname(os.path.dirname(sys.executable))\n",
    "    \n",
    "    # Use CJM config if available, else fallback to env-relative paths\n",
    "    cjm_data_dir = os.environ.get(\"CJM_DATA_DIR\")\n",
    "    cjm_models_dir = os.environ.get(\"CJM_MODELS_DIR\")\n",
    "    \n",
    "    # Plugin data directory\n",
    "    plugin_name = \"cjm-transcription-plugin-voxtral-vllm\"\n",
    "    if cjm_data_dir\n",
    "    \"Return metadata required to register this plugin with the PluginManager.\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333637f6",
   "metadata": {},
   "source": [
    "### Voxtral VLLM Plugin (`plugin.ipynb`)\n",
    "> Plugin implementation for Mistral Voxtral transcription through vLLM server\n",
    "\n",
    "#### Import\n",
    "\n",
    "```python\n",
    "from cjm_transcription_plugin_voxtral_vllm.plugin import (\n",
    "    VLLMServer,\n",
    "    VoxtralVLLMPluginConfig,\n",
    "    VoxtralVLLMPlugin\n",
    ")\n",
    "```\n",
    "\n",
    "#### Functions\n",
    "\n",
    "```python\n",
    "@patch\n",
    "def supports_streaming(\n",
    "    self: VoxtralVLLMPlugin # The plugin instance\n",
    ") -> bool: # True if streaming is supported\n",
    "    \"Check if this plugin supports streaming transcription.\"\n",
    "```\n",
    "\n",
    "```python\n",
    "@patch\n",
    "def execute_stream(\n",
    "    self: VoxtralVLLMPlugin, # The plugin instance\n",
    "    audio: Union[AudioData, str, Path], # Audio data or path to audio file\n",
    "    **kwargs # Additional plugin-specific parameters\n",
    ") -> Generator[str, None, TranscriptionResult]: # Yields text chunks, returns final result\n",
    "    \"Stream transcription results chunk by chunk.\"\n",
    "```\n",
    "\n",
    "#### Classes\n",
    "\n",
    "```python\n",
    "class VLLMServer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"mistralai/Voxtral-Mini-3B-2507\", # Model name to serve\n",
    "        port: int = 8000, # Port for the server\n",
    "        host: str = \"0.0.0.0\", # Host address to bind to\n",
    "        gpu_memory_utilization: float = 0.85, # Fraction of GPU memory to use\n",
    "        log_level: str = \"INFO\", # Logging level (DEBUG, INFO, WARNING, ERROR)\n",
    "        capture_logs: bool = True, # Whether to capture and display server logs\n",
    "        **kwargs # Additional vLLM server arguments\n",
    "    )\n",
    "    \"vLLM server manager for Voxtral models.\"\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            model: str = \"mistralai/Voxtral-Mini-3B-2507\", # Model name to serve\n",
    "            port: int = 8000, # Port for the server\n",
    "            host: str = \"0.0.0.0\", # Host address to bind to\n",
    "            gpu_memory_utilization: float = 0.85, # Fraction of GPU memory to use\n",
    "            log_level: str = \"INFO\", # Logging level (DEBUG, INFO, WARNING, ERROR)\n",
    "            capture_logs: bool = True, # Whether to capture and display server logs\n",
    "            **kwargs # Additional vLLM server arguments\n",
    "        )\n",
    "    \n",
    "    def add_log_callback(\n",
    "            self, \n",
    "            callback: Callable[[str], None] # Function that receives log line strings\n",
    "        ) -> None: # Returns nothing\n",
    "        \"Add a callback function to receive each log line.\"\n",
    "    \n",
    "    def start(\n",
    "            self, \n",
    "            wait_for_ready: bool = True, # Wait for server to be ready before returning\n",
    "            timeout: int = 120, # Maximum seconds to wait for server readiness\n",
    "            show_progress: bool = True # Show progress indicators during startup\n",
    "        ) -> None: # Returns nothing\n",
    "        \"Start the vLLM server.\"\n",
    "    \n",
    "    def stop(self) -> None: # Returns nothing\n",
    "            \"\"\"Stop the vLLM server.\"\"\"\n",
    "            if self.process and self.process.poll() is None\n",
    "        \"Stop the vLLM server.\"\n",
    "    \n",
    "    def restart(self) -> None: # Returns nothing\n",
    "            \"\"\"Restart the server.\"\"\"\n",
    "            self.stop()\n",
    "            time.sleep(2)\n",
    "            self.start()\n",
    "        \n",
    "        def is_running(self) -> bool: # True if server is running and responsive\n",
    "        \"Restart the server.\"\n",
    "    \n",
    "    def is_running(self) -> bool: # True if server is running and responsive\n",
    "        \"Check if server is running and responsive.\"\n",
    "    \n",
    "    def get_recent_logs(\n",
    "            self, \n",
    "            n: int = 100 # Number of recent log lines to retrieve\n",
    "        ) -> List[str]: # List of recent log lines\n",
    "        \"Get the most recent n log lines.\"\n",
    "    \n",
    "    def get_metrics_from_logs(self) -> dict: # Dictionary with performance metrics\n",
    "            \"\"\"Parse recent logs to extract performance metrics.\"\"\"\n",
    "            metrics = {\n",
    "                \"prompt_throughput\": 0.0,\n",
    "        \"Parse recent logs to extract performance metrics.\"\n",
    "    \n",
    "    def tail_logs(\n",
    "            self, \n",
    "            follow: bool = True, # Continue displaying new logs as they arrive\n",
    "            n: int = 10 # Number of initial lines to display\n",
    "        ) -> None: # Returns nothing\n",
    "        \"Tail the server logs (similar to tail -f).\"\n",
    "```\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class VoxtralVLLMPluginConfig:\n",
    "    \"Configuration for Voxtral VLLM transcription plugin.\"\n",
    "    \n",
    "    model_id: str = field(...)\n",
    "    device: str = field(...)\n",
    "    server_mode: str = field(...)\n",
    "    server_url: str = field(...)\n",
    "    server_port: int = field(...)\n",
    "    gpu_memory_utilization: float = field(...)\n",
    "    max_model_len: int = field(...)\n",
    "    language: Optional[str] = field(...)\n",
    "    temperature: float = field(...)\n",
    "    streaming: bool = field(...)\n",
    "    server_startup_timeout: int = field(...)\n",
    "    auto_start_server: bool = field(...)\n",
    "    capture_server_logs: bool = field(...)\n",
    "    dtype: str = field(...)\n",
    "    tensor_parallel_size: int = field(...)\n",
    "```\n",
    "\n",
    "```python\n",
    "class VoxtralVLLMPlugin:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the Voxtral VLLM plugin with default configuration.\"\"\"\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{type(self).__name__}\")\n",
    "        self.config: VoxtralVLLMPluginConfig = None\n",
    "    \"Mistral Voxtral transcription plugin via vLLM server.\"\n",
    "    \n",
    "    def __init__(self):\n",
    "            \"\"\"Initialize the Voxtral VLLM plugin with default configuration.\"\"\"\n",
    "            self.logger = logging.getLogger(f\"{__name__}.{type(self).__name__}\")\n",
    "            self.config: VoxtralVLLMPluginConfig = None\n",
    "        \"Initialize the Voxtral VLLM plugin with default configuration.\"\n",
    "    \n",
    "    def name(self) -> str: # The plugin name identifier\n",
    "            \"\"\"Get the plugin name identifier.\"\"\"\n",
    "            return \"voxtral_vllm\"\n",
    "        \n",
    "        @property\n",
    "        def version(self) -> str: # The plugin version string\n",
    "        \"Get the plugin name identifier.\"\n",
    "    \n",
    "    def version(self) -> str: # The plugin version string\n",
    "            \"\"\"Get the plugin version string.\"\"\"\n",
    "            return \"1.0.0\"\n",
    "        \n",
    "        @property\n",
    "        def supported_formats(self) -> List[str]: # List of supported audio formats\n",
    "        \"Get the plugin version string.\"\n",
    "    \n",
    "    def supported_formats(self) -> List[str]: # List of supported audio formats\n",
    "            \"\"\"Get the list of supported audio file formats.\"\"\"\n",
    "            return [\"wav\", \"mp3\", \"flac\", \"m4a\", \"ogg\", \"webm\", \"mp4\", \"avi\", \"mov\"]\n",
    "        \n",
    "        def get_current_config(self) -> Dict[str, Any]: # Current configuration as dictionary\n",
    "        \"Get the list of supported audio file formats.\"\n",
    "    \n",
    "    def get_current_config(self) -> Dict[str, Any]: # Current configuration as dictionary\n",
    "            \"\"\"Return current configuration state.\"\"\"\n",
    "            if not self.config\n",
    "        \"Return current configuration state.\"\n",
    "    \n",
    "    def get_config_schema(self) -> Dict[str, Any]: # JSON Schema for configuration\n",
    "            \"\"\"Return JSON Schema for UI generation.\"\"\"\n",
    "            return dataclass_to_jsonschema(VoxtralVLLMPluginConfig)\n",
    "    \n",
    "        @staticmethod\n",
    "        def get_config_dataclass() -> VoxtralVLLMPluginConfig: # Configuration dataclass\n",
    "        \"Return JSON Schema for UI generation.\"\n",
    "    \n",
    "    def get_config_dataclass() -> VoxtralVLLMPluginConfig: # Configuration dataclass\n",
    "            \"\"\"Return dataclass describing the plugin's configuration options.\"\"\"\n",
    "            return VoxtralVLLMPluginConfig\n",
    "        \n",
    "        def initialize(\n",
    "            self,\n",
    "            config: Optional[Any] = None # Configuration dataclass, dict, or None\n",
    "        ) -> None\n",
    "        \"Return dataclass describing the plugin's configuration options.\"\n",
    "    \n",
    "    def initialize(\n",
    "            self,\n",
    "            config: Optional[Any] = None # Configuration dataclass, dict, or None\n",
    "        ) -> None\n",
    "        \"Initialize or re-configure the plugin (idempotent).\"\n",
    "    \n",
    "    def execute(\n",
    "            self,\n",
    "            audio: Union[AudioData, str, Path], # Audio data or path to audio file to transcribe\n",
    "            **kwargs # Additional arguments to override config\n",
    "        ) -> TranscriptionResult: # Transcription result with text and metadata\n",
    "        \"Transcribe audio using Voxtral via vLLM.\"\n",
    "    \n",
    "    def is_available(self) -> bool: # True if vLLM and dependencies are available\n",
    "            \"\"\"Check if vLLM and required dependencies are available.\"\"\"\n",
    "            if not OPENAI_AVAILABLE\n",
    "        \"Check if vLLM and required dependencies are available.\"\n",
    "    \n",
    "    def cleanup(self) -> None:\n",
    "            \"\"\"Clean up resources.\"\"\"\n",
    "            self.logger.info(\"Cleaning up Voxtral VLLM plugin\")\n",
    "            \n",
    "            # Stop managed server if running\n",
    "            if self.config and self.config.server_mode == \"managed\" and self.server\n",
    "        \"Clean up resources.\"\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
