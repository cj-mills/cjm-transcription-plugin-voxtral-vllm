{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cjm-transcription-plugin-voxtral-vllm\n",
    "\n",
    "> Mistral Voxtral plugin for the cjm-transcription-plugin-system library - provides local speech-to-text transcription through vLLM with configurable model selection and parameter control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install\n",
    "\n",
    "```bash\n",
    "pip install cjm_transcription_plugin_voxtral_vllm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Structure\n",
    "\n",
    "```\n",
    "nbs/\n",
    "└── plugin.ipynb # Plugin implementation for Mistral Voxtral transcription through vLLM server\n",
    "```\n",
    "\n",
    "Total: 1 notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Dependencies\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    plugin[plugin<br/>Voxtral VLLM Plugin]\n",
    "\n",
    "```\n",
    "\n",
    "No cross-module dependencies detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLI Reference\n",
    "\n",
    "No CLI commands found in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Overview\n",
    "\n",
    "Detailed documentation for each module in the project:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voxtral VLLM Plugin (`plugin.ipynb`)\n",
    "> Plugin implementation for Mistral Voxtral transcription through vLLM server\n",
    "\n",
    "#### Import\n",
    "\n",
    "```python\n",
    "from cjm_transcription_plugin_voxtral_vllm.plugin import (\n",
    "    VLLMServer,\n",
    "    VoxtralVLLMPlugin\n",
    ")\n",
    "```\n",
    "\n",
    "#### Functions\n",
    "\n",
    "```python\n",
    "@patch\n",
    "def supports_streaming(\n",
    "    self: VoxtralVLLMPlugin # The plugin instance\n",
    ") -> bool: # True if streaming is supported\n",
    "    \"Check if this plugin supports streaming transcription.\"\n",
    "```\n",
    "\n",
    "```python\n",
    "@patch\n",
    "def execute_stream(\n",
    "    self: VoxtralVLLMPlugin, # The plugin instance\n",
    "    audio: Union[AudioData, str, Path], # Audio data or path to audio file\n",
    "    **kwargs # Additional plugin-specific parameters\n",
    ") -> Generator[str, None, TranscriptionResult]: # Yields text chunks, returns final result\n",
    "    \"Stream transcription results chunk by chunk.\"\n",
    "```\n",
    "\n",
    "#### Classes\n",
    "\n",
    "```python\n",
    "class VLLMServer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"mistralai/Voxtral-Mini-3B-2507\", # Model name to serve\n",
    "        port: int = 8000, # Port for the server\n",
    "        host: str = \"0.0.0.0\", # Host address to bind to\n",
    "        gpu_memory_utilization: float = 0.85, # Fraction of GPU memory to use\n",
    "        log_level: str = \"INFO\", # Logging level (DEBUG, INFO, WARNING, ERROR)\n",
    "        capture_logs: bool = True, # Whether to capture and display server logs\n",
    "        **kwargs # Additional vLLM server arguments\n",
    "    )\n",
    "    \"vLLM server manager for Voxtral models.\"\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            model: str = \"mistralai/Voxtral-Mini-3B-2507\", # Model name to serve\n",
    "            port: int = 8000, # Port for the server\n",
    "            host: str = \"0.0.0.0\", # Host address to bind to\n",
    "            gpu_memory_utilization: float = 0.85, # Fraction of GPU memory to use\n",
    "            log_level: str = \"INFO\", # Logging level (DEBUG, INFO, WARNING, ERROR)\n",
    "            capture_logs: bool = True, # Whether to capture and display server logs\n",
    "            **kwargs # Additional vLLM server arguments\n",
    "        )\n",
    "    \n",
    "    def add_log_callback(\n",
    "            self, \n",
    "            callback: Callable[[str], None] # Function that receives log line strings\n",
    "        ) -> None: # Returns nothing\n",
    "        \"Add a callback function to receive each log line.\"\n",
    "    \n",
    "    def start(\n",
    "            self, \n",
    "            wait_for_ready: bool = True, # Wait for server to be ready before returning\n",
    "            timeout: int = 120, # Maximum seconds to wait for server readiness\n",
    "            show_progress: bool = True # Show progress indicators during startup\n",
    "        ) -> None: # Returns nothing\n",
    "        \"Start the vLLM server.\"\n",
    "    \n",
    "    def stop(self) -> None: # Returns nothing\n",
    "            \"\"\"Stop the vLLM server.\"\"\"\n",
    "            if self.process and self.process.poll() is None\n",
    "        \"Stop the vLLM server.\"\n",
    "    \n",
    "    def restart(self) -> None: # Returns nothing\n",
    "            \"\"\"Restart the server.\"\"\"\n",
    "            self.stop()\n",
    "            time.sleep(2)\n",
    "            self.start()\n",
    "        \n",
    "        def is_running(self) -> bool: # True if server is running and responsive\n",
    "        \"Restart the server.\"\n",
    "    \n",
    "    def is_running(self) -> bool: # True if server is running and responsive\n",
    "        \"Check if server is running and responsive.\"\n",
    "    \n",
    "    def get_recent_logs(\n",
    "            self, \n",
    "            n: int = 100 # Number of recent log lines to retrieve\n",
    "        ) -> List[str]: # List of recent log lines\n",
    "        \"Get the most recent n log lines.\"\n",
    "    \n",
    "    def get_metrics_from_logs(self) -> dict: # Dictionary with performance metrics\n",
    "            \"\"\"Parse recent logs to extract performance metrics.\"\"\"\n",
    "            metrics = {\n",
    "                \"prompt_throughput\": 0.0,\n",
    "        \"Parse recent logs to extract performance metrics.\"\n",
    "    \n",
    "    def tail_logs(\n",
    "            self, \n",
    "            follow: bool = True, # Continue displaying new logs as they arrive\n",
    "            n: int = 10 # Number of initial lines to display\n",
    "        ) -> None: # Returns nothing\n",
    "        \"Tail the server logs (similar to tail -f).\"\n",
    "```\n",
    "\n",
    "```python\n",
    "class VoxtralVLLMPlugin:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the Voxtral VLLM plugin with default configuration.\"\"\"\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{type(self).__name__}\")\n",
    "        self.config = {}\n",
    "        self.server: Optional[VLLMServer] = None\n",
    "    \"Mistral Voxtral transcription plugin via vLLM server.\"\n",
    "    \n",
    "    def __init__(self):\n",
    "            \"\"\"Initialize the Voxtral VLLM plugin with default configuration.\"\"\"\n",
    "            self.logger = logging.getLogger(f\"{__name__}.{type(self).__name__}\")\n",
    "            self.config = {}\n",
    "            self.server: Optional[VLLMServer] = None\n",
    "        \"Initialize the Voxtral VLLM plugin with default configuration.\"\n",
    "    \n",
    "    def name(self) -> str: # The plugin name identifier\n",
    "            \"\"\"Get the plugin name identifier.\"\"\"\n",
    "            return \"voxtral_vllm\"\n",
    "        \n",
    "        @property\n",
    "        def version(self) -> str: # The plugin version string\n",
    "        \"Get the plugin name identifier.\"\n",
    "    \n",
    "    def version(self) -> str: # The plugin version string\n",
    "            \"\"\"Get the plugin version string.\"\"\"\n",
    "            return \"1.0.0\"\n",
    "        \n",
    "        @property\n",
    "        def supported_formats(self) -> List[str]: # List of supported audio formats\n",
    "        \"Get the plugin version string.\"\n",
    "    \n",
    "    def supported_formats(self) -> List[str]: # List of supported audio formats\n",
    "            \"\"\"Get the list of supported audio file formats.\"\"\"\n",
    "            return [\"wav\", \"mp3\", \"flac\", \"m4a\", \"ogg\", \"webm\", \"mp4\", \"avi\", \"mov\"]\n",
    "    \n",
    "        @staticmethod\n",
    "        def get_config_schema() -> Dict[str, Any]: # Configuration schema dictionary\n",
    "        \"Get the list of supported audio file formats.\"\n",
    "    \n",
    "    def get_config_schema() -> Dict[str, Any]: # Configuration schema dictionary\n",
    "            \"\"\"Return configuration schema for Voxtral VLLM.\"\"\"\n",
    "            return {\n",
    "                \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "        \"Return configuration schema for Voxtral VLLM.\"\n",
    "    \n",
    "    def get_current_config(self) -> Dict[str, Any]: # Current configuration dictionary\n",
    "            \"\"\"Return current configuration.\"\"\"\n",
    "            defaults = self.get_config_defaults()\n",
    "            return {**defaults, **self.config}\n",
    "        \n",
    "        def initialize(\n",
    "            self,\n",
    "            config: Optional[Dict[str, Any]] = None # Configuration dictionary to initialize the plugin\n",
    "        ) -> None: # Returns nothing\n",
    "        \"Return current configuration.\"\n",
    "    \n",
    "    def initialize(\n",
    "            self,\n",
    "            config: Optional[Dict[str, Any]] = None # Configuration dictionary to initialize the plugin\n",
    "        ) -> None: # Returns nothing\n",
    "        \"Initialize the plugin with configuration.\"\n",
    "    \n",
    "    def execute(\n",
    "            self,\n",
    "            audio: Union[AudioData, str, Path], # Audio data or path to audio file to transcribe\n",
    "            **kwargs # Additional arguments to override config\n",
    "        ) -> TranscriptionResult: # Transcription result with text and metadata\n",
    "        \"Transcribe audio using Voxtral via vLLM.\"\n",
    "    \n",
    "    def is_available(self) -> bool: # True if vLLM and dependencies are available\n",
    "            \"\"\"Check if vLLM and required dependencies are available.\"\"\"\n",
    "            if not OPENAI_AVAILABLE\n",
    "        \"Check if vLLM and required dependencies are available.\"\n",
    "    \n",
    "    def cleanup(self) -> None: # Returns nothing\n",
    "            \"\"\"Clean up resources.\"\"\"\n",
    "            self.logger.info(\"Cleaning up Voxtral VLLM plugin\")\n",
    "            \n",
    "            # Stop managed server if running\n",
    "            if self.config.get(\"server_mode\") == \"managed\" and self.server\n",
    "        \"Clean up resources.\"\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
