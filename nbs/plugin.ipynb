{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e337c1ba",
   "metadata": {},
   "source": [
    "# Voxtral VLLM Plugin\n",
    "\n",
    "> Plugin implementation for Mistral Voxtral transcription through vLLM server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6262b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c18907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Union, Generator\n",
    "import tempfile\n",
    "import warnings\n",
    "from threading import Thread\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import atexit\n",
    "import signal\n",
    "import threading\n",
    "import queue\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "from fastcore.basics import patch\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    OPENAI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPENAI_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from mistral_common.protocol.transcription.request import TranscriptionRequest\n",
    "    from mistral_common.protocol.instruct.chunk import RawAudio\n",
    "    from mistral_common.audio import Audio\n",
    "    MISTRAL_COMMON_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MISTRAL_COMMON_AVAILABLE = False\n",
    "    \n",
    "from cjm_transcription_plugin_system.plugin_interface import PluginInterface\n",
    "from cjm_transcription_plugin_system.core import AudioData, TranscriptionResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import atexit\n",
    "import signal\n",
    "import threading\n",
    "import queue\n",
    "import re\n",
    "from contextlib import contextmanager\n",
    "from typing import Optional, Callable, List\n",
    "from datetime import datetime\n",
    "\n",
    "class VLLMServer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"mistralai/Voxtral-Mini-3B-2507\",\n",
    "        port: int = 8000,\n",
    "        host: str = \"0.0.0.0\",\n",
    "        gpu_memory_utilization: float = 0.85,\n",
    "        log_level: str = \"INFO\",  # DEBUG, INFO, WARNING, ERROR\n",
    "        capture_logs: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.port = port\n",
    "        self.host = host\n",
    "        self.base_url = f\"http://localhost:{self.port}\"\n",
    "        self.process: Optional[subprocess.Popen] = None\n",
    "        self.capture_logs = capture_logs\n",
    "        self.log_level = log_level\n",
    "        \n",
    "        # Log management\n",
    "        self.log_queue = queue.Queue()\n",
    "        self.log_thread = None\n",
    "        self.stop_logging = threading.Event()\n",
    "        self.log_callbacks: List[Callable] = []\n",
    "        \n",
    "        # Build command\n",
    "        self.cmd = [\n",
    "            \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "            \"--model\", model,\n",
    "            \"--port\", str(self.port),\n",
    "            \"--host\", host,\n",
    "            \"--gpu-memory-utilization\", str(gpu_memory_utilization),\n",
    "            \"--tokenizer-mode\", \"mistral\",\n",
    "            \"--config-format\", \"mistral\",\n",
    "            \"--load-format\", \"mistral\",\n",
    "        ]\n",
    "        \n",
    "        # Add any additional arguments\n",
    "        for key, value in kwargs.items():\n",
    "            self.cmd.extend([f\"--{key.replace('_', '-')}\", str(value)])\n",
    "    \n",
    "    def add_log_callback(self, callback: Callable[[str], None]):\n",
    "        \"\"\"Add a callback function that will be called for each log line.\n",
    "        \n",
    "        Args:\n",
    "            callback: Function that takes a log line string as input\n",
    "        \"\"\"\n",
    "        self.log_callbacks.append(callback)\n",
    "    \n",
    "    def _process_log_line(self, line: str):\n",
    "        \"\"\"Process a single log line.\"\"\"\n",
    "        if not line.strip():\n",
    "            return\n",
    "        \n",
    "        # Add timestamp if not present\n",
    "        if not re.match(r'^\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', line):\n",
    "            timestamp = datetime.now().strftime(\"%m-%d %H:%M:%S\")\n",
    "            line = f\"{timestamp} {line}\"\n",
    "        \n",
    "        # Store in queue for retrieval\n",
    "        self.log_queue.put(line)\n",
    "        \n",
    "        # Call callbacks\n",
    "        for callback in self.log_callbacks:\n",
    "            try:\n",
    "                callback(line)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in log callback: {e}\")\n",
    "        \n",
    "        # Print if we're capturing logs\n",
    "        if self.capture_logs:\n",
    "            # Color code based on log level\n",
    "            if \"ERROR\" in line:\n",
    "                print(f\"\\033[91m{line}\\033[0m\")  # Red\n",
    "            elif \"WARNING\" in line:\n",
    "                print(f\"\\033[93m{line}\\033[0m\")  # Yellow\n",
    "            elif \"INFO\" in line:\n",
    "                print(f\"\\033[94m{line}\\033[0m\")  # Blue\n",
    "            else:\n",
    "                print(line)\n",
    "    \n",
    "    def _log_reader(self, pipe, pipe_name: str):\n",
    "        \"\"\"Read logs from a pipe in a separate thread.\"\"\"\n",
    "        for line in iter(pipe.readline, ''):\n",
    "            if self.stop_logging.is_set():\n",
    "                break\n",
    "            if line:\n",
    "                # Add pipe identifier for debugging\n",
    "                if pipe_name == \"stderr\" and \"INFO\" in line:\n",
    "                    # Most INFO logs come through stderr in vLLM\n",
    "                    self._process_log_line(line.strip())\n",
    "                elif pipe_name == \"stdout\":\n",
    "                    self._process_log_line(line.strip())\n",
    "    \n",
    "    def start(self, wait_for_ready: bool = True, timeout: int = 120, show_progress: bool = True):\n",
    "        \"\"\"Start the vLLM server.\n",
    "        \n",
    "        Args:\n",
    "            wait_for_ready: Wait for server to be ready before returning\n",
    "            timeout: Maximum time to wait for server to be ready\n",
    "            show_progress: Show progress indicators during startup\n",
    "        \"\"\"\n",
    "        if self.is_running():\n",
    "            print(\"Server is already running\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Starting vLLM server with model {self.model}...\")\n",
    "        \n",
    "        # Start process with pipes for output capture\n",
    "        self.process = subprocess.Popen(\n",
    "            self.cmd,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1,  # Line buffered\n",
    "            universal_newlines=True,\n",
    "            preexec_fn=lambda: signal.signal(signal.SIGINT, signal.SIG_IGN)\n",
    "        )\n",
    "        \n",
    "        # Start log reading threads\n",
    "        self.stop_logging.clear()\n",
    "        \n",
    "        if self.capture_logs:\n",
    "            # Create threads for reading stdout and stderr\n",
    "            stdout_thread = threading.Thread(\n",
    "                target=self._log_reader,\n",
    "                args=(self.process.stdout, \"stdout\"),\n",
    "                daemon=True\n",
    "            )\n",
    "            stderr_thread = threading.Thread(\n",
    "                target=self._log_reader,\n",
    "                args=(self.process.stderr, \"stderr\"),\n",
    "                daemon=True\n",
    "            )\n",
    "            stdout_thread.start()\n",
    "            stderr_thread.start()\n",
    "        \n",
    "        # Register cleanup on exit\n",
    "        atexit.register(self.stop)\n",
    "        \n",
    "        if wait_for_ready:\n",
    "            self._wait_for_server(timeout, show_progress)\n",
    "    \n",
    "    def _wait_for_server(self, timeout: int, show_progress: bool):\n",
    "        \"\"\"Wait for server to be ready to accept requests.\"\"\"\n",
    "        start_time = time.time()\n",
    "        last_status = \"\"\n",
    "        \n",
    "        # Key phases to look for in logs\n",
    "        phases = {\n",
    "            \"detected platform\": \"üîç Detecting platform...\",\n",
    "            \"Loading model\": \"üì¶ Loading model weights...\",\n",
    "            \"downloading weights\": \"‚¨áÔ∏è Downloading model weights...\",\n",
    "            \"Loading weights took\": \"‚úÖ Model weights loaded\",\n",
    "            \"Graph capturing\": \"üìä Capturing CUDA graphs...\",\n",
    "            \"Graph capturing finished\": \"‚úÖ CUDA graphs ready\",\n",
    "            \"Starting vLLM API server\": \"üöÄ Starting API server...\",\n",
    "            \"Available routes\": \"‚úÖ Server ready!\"\n",
    "        }\n",
    "        \n",
    "        while time.time() - start_time < timeout:\n",
    "            # Check for new status in logs\n",
    "            if show_progress and not self.log_queue.empty():\n",
    "                try:\n",
    "                    log_line = self.log_queue.get_nowait()\n",
    "                    for key, status_msg in phases.items():\n",
    "                        if key in log_line:\n",
    "                            if status_msg != last_status:\n",
    "                                print(f\"\\n  {status_msg}\")\n",
    "                                last_status = status_msg\n",
    "                            break\n",
    "                except queue.Empty:\n",
    "                    pass\n",
    "            \n",
    "            # Try to connect to the server\n",
    "            try:\n",
    "                response = requests.get(f\"{self.base_url}/health\", timeout=1)\n",
    "                if response.status_code == 200:\n",
    "                    print(f\"\\n‚úÖ vLLM server is ready at {self.base_url}\")\n",
    "                    return\n",
    "            except requests.exceptions.RequestException:\n",
    "                pass\n",
    "            \n",
    "            # Check if process has crashed\n",
    "            if self.process.poll() is not None:\n",
    "                raise RuntimeError(f\"Server process exited with code {self.process.poll()}\")\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        raise TimeoutError(f\"Server did not start within {timeout} seconds\")\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop the vLLM server.\"\"\"\n",
    "        if self.process and self.process.poll() is None:\n",
    "            print(\"Stopping vLLM server...\")\n",
    "            \n",
    "            # Signal threads to stop\n",
    "            self.stop_logging.set()\n",
    "            \n",
    "            # Terminate process\n",
    "            self.process.terminate()\n",
    "            try:\n",
    "                self.process.wait(timeout=10)\n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(\"Force killing server...\")\n",
    "                self.process.kill()\n",
    "                self.process.wait()\n",
    "            \n",
    "            self.process = None\n",
    "            print(\"Server stopped\")\n",
    "    \n",
    "    def restart(self):\n",
    "        \"\"\"Restart the server.\"\"\"\n",
    "        self.stop()\n",
    "        time.sleep(2)\n",
    "        self.start()\n",
    "    \n",
    "    def is_running(self) -> bool:\n",
    "        \"\"\"Check if server is running and responsive.\n",
    "        \n",
    "        This method checks both if the process is alive and if the server\n",
    "        is actually responding to health checks.\n",
    "        \"\"\"\n",
    "        # First check if process exists and hasn't exited\n",
    "        if self.process is None or self.process.poll() is not None:\n",
    "            return False\n",
    "        \n",
    "        # Try to connect to the server\n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}/health\", timeout=1)\n",
    "            if response.status_code == 200:\n",
    "                # print(f\"\\n‚úÖ vLLM server is ready at {self.base_url}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"\\n‚ùå vLLM server is not ready at {self.base_url}\")\n",
    "                return False\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\n‚ùå The following exception occurred: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_recent_logs(self, n: int = 100) -> List[str]:\n",
    "        \"\"\"Get the most recent n log lines.\n",
    "        \n",
    "        Args:\n",
    "            n: Number of recent log lines to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            List of recent log lines\n",
    "        \"\"\"\n",
    "        logs = []\n",
    "        while not self.log_queue.empty() and len(logs) < n:\n",
    "            try:\n",
    "                logs.append(self.log_queue.get_nowait())\n",
    "            except queue.Empty:\n",
    "                break\n",
    "        return logs\n",
    "    \n",
    "    def get_metrics_from_logs(self) -> dict:\n",
    "        \"\"\"Parse recent logs to extract performance metrics.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with metrics like throughput, GPU usage, etc.\n",
    "        \"\"\"\n",
    "        metrics = {\n",
    "            \"prompt_throughput\": 0.0,\n",
    "            \"generation_throughput\": 0.0,\n",
    "            \"running_requests\": 0,\n",
    "            \"waiting_requests\": 0,\n",
    "            \"gpu_kv_cache_usage\": 0.0,\n",
    "            \"prefix_cache_hit_rate\": 0.0,\n",
    "        }\n",
    "        \n",
    "        # Look for metric lines in recent logs\n",
    "        recent_logs = self.get_recent_logs(50)\n",
    "        for log in recent_logs:\n",
    "            if \"Avg prompt throughput:\" in log:\n",
    "                # Parse metrics line\n",
    "                match = re.search(r'Avg prompt throughput: ([\\d.]+) tokens/s', log)\n",
    "                if match:\n",
    "                    metrics[\"prompt_throughput\"] = float(match.group(1))\n",
    "                \n",
    "                match = re.search(r'Avg generation throughput: ([\\d.]+) tokens/s', log)\n",
    "                if match:\n",
    "                    metrics[\"generation_throughput\"] = float(match.group(1))\n",
    "                \n",
    "                match = re.search(r'Running: (\\d+) reqs', log)\n",
    "                if match:\n",
    "                    metrics[\"running_requests\"] = int(match.group(1))\n",
    "                \n",
    "                match = re.search(r'Waiting: (\\d+) reqs', log)\n",
    "                if match:\n",
    "                    metrics[\"waiting_requests\"] = int(match.group(1))\n",
    "                \n",
    "                match = re.search(r'GPU KV cache usage: ([\\d.]+)%', log)\n",
    "                if match:\n",
    "                    metrics[\"gpu_kv_cache_usage\"] = float(match.group(1))\n",
    "                \n",
    "                match = re.search(r'Prefix cache hit rate: ([\\d.]+)%', log)\n",
    "                if match:\n",
    "                    metrics[\"prefix_cache_hit_rate\"] = float(match.group(1))\n",
    "                \n",
    "                break  # Use the most recent metrics\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def tail_logs(self, follow: bool = True, n: int = 10):\n",
    "        \"\"\"Tail the server logs (similar to tail -f).\n",
    "        \n",
    "        Args:\n",
    "            follow: Continue displaying new logs as they arrive\n",
    "            n: Number of initial lines to display\n",
    "        \"\"\"\n",
    "        # Display recent logs\n",
    "        recent = self.get_recent_logs(n)\n",
    "        for line in recent:\n",
    "            print(line)\n",
    "        \n",
    "        if follow:\n",
    "            print(\"\\n--- Following logs (Ctrl+C to stop) ---\\n\")\n",
    "            try:\n",
    "                while self.is_running():\n",
    "                    if not self.log_queue.empty():\n",
    "                        print(self.log_queue.get())\n",
    "                    else:\n",
    "                        time.sleep(0.1)\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n--- Stopped following logs ---\")\n",
    "    \n",
    "    def __enter__(self):\n",
    "        \"\"\"Context manager entry.\"\"\"\n",
    "        self.start()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Context manager exit.\"\"\"\n",
    "        self.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff4bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VoxtralVLLMPlugin(PluginInterface):\n",
    "    \"\"\"Mistral Voxtral transcription plugin via vLLM server.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the Voxtral VLLM plugin with default configuration.\"\"\"\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{type(self).__name__}\")\n",
    "        self.config = {}\n",
    "        self.server: Optional[VLLMServer] = None\n",
    "        self.client: Optional[OpenAI] = None\n",
    "        self.model_id: Optional[str] = None\n",
    "    \n",
    "    @property\n",
    "    def name(\n",
    "        self\n",
    "    ) -> str:  # Returns the plugin name\n",
    "        \"\"\"Get the plugin name identifier.\"\"\"\n",
    "        return \"voxtral_vllm\"\n",
    "    \n",
    "    @property\n",
    "    def version(\n",
    "        self\n",
    "    ) -> str:  # Returns the plugin version\n",
    "        \"\"\"Get the plugin version string.\"\"\"\n",
    "        return \"1.0.0\"\n",
    "    \n",
    "    @property\n",
    "    def supported_formats(\n",
    "        self\n",
    "    ) -> List[str]:  # Returns list of supported audio formats\n",
    "        \"\"\"Get the list of supported audio file formats.\"\"\"\n",
    "        return [\"wav\", \"mp3\", \"flac\", \"m4a\", \"ogg\", \"webm\", \"mp4\", \"avi\", \"mov\"]\n",
    "    \n",
    "    def get_config_schema(\n",
    "        self\n",
    "    ) -> Dict[str, Any]:  # Returns the configuration schema dictionary\n",
    "        \"\"\"Return configuration schema for Voxtral VLLM.\"\"\"\n",
    "        return {\n",
    "            \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "            \"type\": \"object\",\n",
    "            \"title\": \"Voxtral VLLM Configuration\",\n",
    "            \"properties\": {\n",
    "                \"model_id\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"mistralai/Voxtral-Mini-3B-2507\", \"mistralai/Voxtral-Small-24B-2507\"],\n",
    "                    \"default\": \"mistralai/Voxtral-Mini-3B-2507\",\n",
    "                    \"description\": \"Voxtral model to use. Mini is faster, Small is more accurate.\"\n",
    "                },\n",
    "                \"server_mode\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"managed\", \"external\"],\n",
    "                    \"default\": \"managed\",\n",
    "                    \"description\": \"'managed': plugin manages server lifecycle, 'external': connect to existing server\"\n",
    "                },\n",
    "                \"server_url\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"default\": \"http://localhost:8000\",\n",
    "                    \"description\": \"vLLM server URL (for external mode)\"\n",
    "                },\n",
    "                \"server_port\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"minimum\": 1024,\n",
    "                    \"maximum\": 65535,\n",
    "                    \"default\": 8000,\n",
    "                    \"description\": \"Port for managed vLLM server\"\n",
    "                },\n",
    "                \"gpu_memory_utilization\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"minimum\": 0.1,\n",
    "                    \"maximum\": 1.0,\n",
    "                    \"default\": 0.85,\n",
    "                    \"description\": \"Fraction of GPU memory to use (managed mode)\"\n",
    "                },\n",
    "                \"max_model_len\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"minimum\": 1024,\n",
    "                    \"maximum\": 131072,\n",
    "                    \"default\": 32768,\n",
    "                    \"description\": \"Maximum sequence length for the model\"\n",
    "                },\n",
    "                \"language\": {\n",
    "                    \"type\": [\"string\", \"null\"],\n",
    "                    \"default\": \"en\",\n",
    "                    \"description\": \"Language code for transcription (e.g., 'en', 'es', 'fr')\",\n",
    "                    \"examples\": ['ar', 'nl', 'en', 'fr', 'de', 'hi', 'it', 'pt', 'es']\n",
    "                },\n",
    "                \"temperature\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"minimum\": 0.0,\n",
    "                    \"maximum\": 2.0,\n",
    "                    \"default\": 0.0,\n",
    "                    \"description\": \"Temperature for sampling (0.0 for deterministic)\"\n",
    "                },\n",
    "                \"streaming\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": False,\n",
    "                    \"description\": \"Enable streaming output by default\"\n",
    "                },\n",
    "                \"server_startup_timeout\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"minimum\": 30,\n",
    "                    \"maximum\": 600,\n",
    "                    \"default\": 120,\n",
    "                    \"description\": \"Timeout in seconds for server startup (managed mode)\"\n",
    "                },\n",
    "                \"auto_start_server\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": True,\n",
    "                    \"description\": \"Automatically start server on first use (managed mode)\"\n",
    "                },\n",
    "                \"capture_server_logs\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": True,\n",
    "                    \"description\": \"Capture vLLM server logs (managed mode)\"\n",
    "                },\n",
    "                \"dtype\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"auto\", \"half\", \"float16\", \"bfloat16\", \"float32\"],\n",
    "                    \"default\": \"auto\",\n",
    "                    \"description\": \"Data type for model weights\"\n",
    "                },\n",
    "                \"tensor_parallel_size\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"minimum\": 1,\n",
    "                    \"maximum\": 8,\n",
    "                    \"default\": 1,\n",
    "                    \"description\": \"Number of GPUs for tensor parallelism\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"model_id\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    \n",
    "    def get_current_config(\n",
    "        self\n",
    "    ) -> Dict[str, Any]:  # Returns the current configuration dictionary\n",
    "        \"\"\"Return current configuration.\"\"\"\n",
    "        defaults = self.get_config_defaults()\n",
    "        return {**defaults, **self.config}\n",
    "    \n",
    "    def initialize(\n",
    "        self,\n",
    "        config: Optional[Dict[str, Any]] = None  # Configuration dictionary to initialize the plugin\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the plugin with configuration.\"\"\"\n",
    "        if config:\n",
    "            is_valid, error = self.validate_config(config)\n",
    "            if not is_valid:\n",
    "                raise ValueError(f\"Invalid configuration: {error}\")\n",
    "        \n",
    "        # Merge with defaults\n",
    "        defaults = self.get_config_defaults()\n",
    "        self.config = {**defaults, **(config or {})}\n",
    "        \n",
    "        self.model_id = self.config[\"model_id\"]\n",
    "        \n",
    "        # Initialize based on server mode\n",
    "        if self.config[\"server_mode\"] == \"managed\":\n",
    "            # Create managed server instance (but don't start yet)\n",
    "            self.server = VLLMServer(\n",
    "                model=self.model_id,\n",
    "                port=self.config[\"server_port\"],\n",
    "                gpu_memory_utilization=self.config[\"gpu_memory_utilization\"],\n",
    "                max_model_len=self.config[\"max_model_len\"],\n",
    "                capture_logs=self.config[\"capture_server_logs\"],\n",
    "                dtype=self.config[\"dtype\"],\n",
    "                tensor_parallel_size=self.config[\"tensor_parallel_size\"]\n",
    "            )\n",
    "            server_url = f\"http://localhost:{self.config['server_port']}\"\n",
    "        else:\n",
    "            # External server mode\n",
    "            server_url = self.config[\"server_url\"]\n",
    "        \n",
    "        # Create OpenAI client\n",
    "        self.client = OpenAI(\n",
    "            api_key=\"EMPTY\",  # vLLM doesn't require API key\n",
    "            base_url=f\"{server_url}/v1\"\n",
    "        )\n",
    "        \n",
    "        self.logger.info(\n",
    "            f\"Initialized Voxtral VLLM plugin with model '{self.model_id}' \"\n",
    "            f\"in {self.config['server_mode']} mode\"\n",
    "        )\n",
    "    \n",
    "    def _ensure_server_running(self) -> None:\n",
    "        \"\"\"Ensure the vLLM server is running (for managed mode).\"\"\"\n",
    "        if self.config[\"server_mode\"] == \"managed\" and self.server:\n",
    "            if not self.server.is_running():\n",
    "                print(\"\\n\\nSERVER IS NOT RUNNING\\n\\n\")\n",
    "                if self.config[\"auto_start_server\"]:\n",
    "                    self.logger.info(\"Starting vLLM server...\")\n",
    "                    self.server.start(\n",
    "                        wait_for_ready=True,\n",
    "                        timeout=self.config[\"server_startup_timeout\"],\n",
    "                        show_progress=self.config.get(\"capture_server_logs\", True)\n",
    "                    )\n",
    "                else:\n",
    "                    raise RuntimeError(\"vLLM server is not running and auto_start_server is disabled\")\n",
    "        elif self.config[\"server_mode\"] == \"external\":\n",
    "            # Check if external server is accessible\n",
    "            try:\n",
    "                response = requests.get(f\"{self.config['server_url']}/health\", timeout=5)\n",
    "                if response.status_code != 200:\n",
    "                    raise RuntimeError(f\"External vLLM server returned status {response.status_code}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                raise RuntimeError(f\"Cannot connect to external vLLM server: {e}\")\n",
    "    \n",
    "    def _prepare_audio(\n",
    "        self,\n",
    "        audio: Union[AudioData, str, Path]  # Audio data, file path, or Path object to prepare\n",
    "    ) -> str:  # Returns path to the prepared audio file\n",
    "        \"\"\"Prepare audio for Voxtral processing.\"\"\"\n",
    "        if isinstance(audio, (str, Path)):\n",
    "            # Already a file path\n",
    "            return str(audio)\n",
    "        \n",
    "        elif isinstance(audio, AudioData):\n",
    "            # Save AudioData to temporary file\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_file:\n",
    "                # Ensure audio is in the correct format\n",
    "                audio_array = audio.samples\n",
    "                \n",
    "                # If stereo, convert to mono\n",
    "                if audio_array.ndim > 1:\n",
    "                    audio_array = audio_array.mean(axis=1)\n",
    "                \n",
    "                # Ensure float32 and normalized\n",
    "                if audio_array.dtype != np.float32:\n",
    "                    audio_array = audio_array.astype(np.float32)\n",
    "                \n",
    "                # Normalize if needed\n",
    "                if audio_array.max() > 1.0:\n",
    "                    audio_array = audio_array / np.abs(audio_array).max()\n",
    "                \n",
    "                # Save to file\n",
    "                sf.write(tmp_file.name, audio_array, audio.sample_rate)\n",
    "                return tmp_file.name\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported audio input type: {type(audio)}\")\n",
    "    \n",
    "    def execute(\n",
    "        self,\n",
    "        audio: Union[AudioData, str, Path],  # Audio data or path to audio file to transcribe\n",
    "        **kwargs  # Additional arguments to override config\n",
    "    ) -> TranscriptionResult:  # Returns transcription result with text and metadata\n",
    "        \"\"\"Transcribe audio using Voxtral via vLLM.\"\"\"\n",
    "        # Ensure server is running\n",
    "        self._ensure_server_running()\n",
    "        \n",
    "        # Prepare audio file\n",
    "        audio_path = self._prepare_audio(audio)\n",
    "        temp_file_created = not isinstance(audio, (str, Path))\n",
    "        \n",
    "        try:\n",
    "            # Merge runtime kwargs with config\n",
    "            exec_config = {**self.config, **kwargs}\n",
    "            \n",
    "            # Prepare inputs using mistral_common\n",
    "            self.logger.info(f\"Processing audio with Voxtral {self.model_id} via vLLM\")\n",
    "            \n",
    "            input_audio = Audio.from_file(audio_path, strict=False)\n",
    "            input_audio = RawAudio.from_audio(input_audio)\n",
    "            \n",
    "            req = TranscriptionRequest(\n",
    "                model=self.model_id,\n",
    "                audio=input_audio,\n",
    "                language=exec_config.get(\"language\", \"en\"),\n",
    "                temperature=exec_config.get(\"temperature\", 0.0)\n",
    "            ).to_openai(exclude=(\"top_p\", \"seed\"))\n",
    "            \n",
    "            # Get transcription from vLLM server\n",
    "            response = self.client.audio.transcriptions.create(**req)\n",
    "            \n",
    "            # Create transcription result\n",
    "            transcription_result = TranscriptionResult(\n",
    "                text=response.text.strip(),\n",
    "                confidence=None,  # vLLM doesn't provide confidence scores\n",
    "                segments=None,  # vLLM doesn't provide segments by default\n",
    "                metadata={\n",
    "                    \"model\": self.model_id,\n",
    "                    \"language\": exec_config.get(\"language\", \"en\"),\n",
    "                    \"server_mode\": self.config[\"server_mode\"],\n",
    "                    \"temperature\": exec_config.get(\"temperature\", 0.0),\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f\"Transcription completed: {len(response.text.split())} words\")\n",
    "            return transcription_result\n",
    "            \n",
    "        finally:\n",
    "            # Clean up temporary file if created\n",
    "            if temp_file_created:\n",
    "                try:\n",
    "                    Path(audio_path).unlink()\n",
    "                except Exception:\n",
    "                    pass\n",
    "    \n",
    "    def is_available(\n",
    "        self\n",
    "    ) -> bool:  # Returns True if vLLM and its dependencies are available\n",
    "        \"\"\"Check if vLLM and required dependencies are available.\"\"\"\n",
    "        if not OPENAI_AVAILABLE:\n",
    "            return False\n",
    "        if not MISTRAL_COMMON_AVAILABLE:\n",
    "            return False\n",
    "        \n",
    "        # Check if vLLM is installed\n",
    "        try:\n",
    "            import vllm\n",
    "            return True\n",
    "        except ImportError:\n",
    "            return False\n",
    "    \n",
    "    def cleanup(\n",
    "        self\n",
    "    ) -> None:\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        self.logger.info(\"Cleaning up Voxtral VLLM plugin\")\n",
    "        \n",
    "        # Stop managed server if running\n",
    "        if self.config.get(\"server_mode\") == \"managed\" and self.server:\n",
    "            if self.server.is_running():\n",
    "                self.logger.info(\"Stopping managed vLLM server\")\n",
    "                self.server.stop()\n",
    "            self.server = None\n",
    "        \n",
    "        # Clear client\n",
    "        self.client = None\n",
    "        \n",
    "        self.logger.info(\"Cleanup completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef2767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def supports_streaming(\n",
    "    self: VoxtralVLLMPlugin\n",
    ") -> bool:\n",
    "    \"\"\"Check if this plugin supports streaming transcription.\"\"\"\n",
    "    return True\n",
    "\n",
    "@patch\n",
    "def execute_stream(\n",
    "    self: VoxtralVLLMPlugin,\n",
    "    audio: Union[AudioData, str, Path],  # Audio data or path to audio file\n",
    "    **kwargs  # Additional plugin-specific parameters\n",
    ") -> Generator[str, None, TranscriptionResult]:  # Yields text chunks, returns final result\n",
    "    \"\"\"Stream transcription results chunk by chunk.\n",
    "    \n",
    "    Args:\n",
    "        audio: Audio data or path to audio file\n",
    "        **kwargs: Additional plugin-specific parameters\n",
    "        \n",
    "    Yields:\n",
    "        str: Partial transcription text chunks as they become available\n",
    "        \n",
    "    Returns:\n",
    "        TranscriptionResult: Final complete transcription with metadata\n",
    "    \"\"\"\n",
    "    # Ensure server is running\n",
    "    self._ensure_server_running()\n",
    "    \n",
    "    # Prepare audio file\n",
    "    audio_path = self._prepare_audio(audio)\n",
    "    temp_file_created = not isinstance(audio, (str, Path))\n",
    "    \n",
    "    try:\n",
    "        # Merge runtime kwargs with config\n",
    "        exec_config = {**self.config, **kwargs}\n",
    "        \n",
    "        # Prepare inputs using mistral_common\n",
    "        self.logger.info(f\"Streaming transcription with Voxtral {self.model_id} via vLLM\")\n",
    "        \n",
    "        input_audio = Audio.from_file(audio_path, strict=False)\n",
    "        input_audio = RawAudio.from_audio(input_audio)\n",
    "        \n",
    "        req = TranscriptionRequest(\n",
    "            model=self.model_id,\n",
    "            audio=input_audio,\n",
    "            language=exec_config.get(\"language\", \"en\"),\n",
    "            temperature=exec_config.get(\"temperature\", 0.0)\n",
    "        ).to_openai(exclude=(\"top_p\", \"seed\"))\n",
    "        \n",
    "        # Get streaming transcription from vLLM server\n",
    "        response = self.client.audio.transcriptions.create(**req, stream=True)\n",
    "        \n",
    "        # Collect generated text\n",
    "        generated_text = \"\"\n",
    "        for chunk in response:\n",
    "            if chunk.choices[0]['delta']['content']:\n",
    "                text_chunk = chunk.choices[0]['delta']['content']\n",
    "                generated_text += text_chunk\n",
    "                yield text_chunk\n",
    "        \n",
    "        # Return final result\n",
    "        return TranscriptionResult(\n",
    "            text=generated_text.strip(),\n",
    "            confidence=None,\n",
    "            segments=None,\n",
    "            metadata={\n",
    "                \"model\": self.model_id,\n",
    "                \"language\": exec_config.get(\"language\", \"en\"),\n",
    "                \"server_mode\": self.config[\"server_mode\"],\n",
    "                \"temperature\": exec_config.get(\"temperature\", 0.0),\n",
    "                \"streaming\": True,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    finally:\n",
    "        # Clean up temporary file if created\n",
    "        if temp_file_created:\n",
    "            try:\n",
    "                Path(audio_path).unlink()\n",
    "            except Exception:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec7ebca",
   "metadata": {},
   "source": [
    "## Testing the Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbbf470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voxtral VLLM available: True\n",
      "Plugin name: voxtral_vllm\n",
      "Plugin version: 1.0.0\n",
      "Supported formats: ['wav', 'mp3', 'flac', 'm4a', 'ogg', 'webm', 'mp4', 'avi', 'mov']\n",
      "Supports streaming: True\n"
     ]
    }
   ],
   "source": [
    "# Test basic functionality\n",
    "plugin = VoxtralVLLMPlugin()\n",
    "\n",
    "# Check availability\n",
    "print(f\"Voxtral VLLM available: {plugin.is_available()}\")\n",
    "print(f\"Plugin name: {plugin.name}\")\n",
    "print(f\"Plugin version: {plugin.version}\")\n",
    "print(f\"Supported formats: {plugin.supported_formats}\")\n",
    "print(f\"Supports streaming: {plugin.supports_streaming()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb1519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "  - mistralai/Voxtral-Mini-3B-2507\n",
      "  - mistralai/Voxtral-Small-24B-2507\n",
      "\n",
      "Server modes: ['managed', 'external']\n"
     ]
    }
   ],
   "source": [
    "# Test configuration schema\n",
    "schema = plugin.get_config_schema()\n",
    "print(\"Available models:\")\n",
    "for model in schema[\"properties\"][\"model_id\"][\"enum\"]:\n",
    "    print(f\"  - {model}\")\n",
    "print(f\"\\nServer modes: {schema['properties']['server_mode']['enum']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd4482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid config: Valid=True\n",
      "Invalid model: Valid=False\n",
      "  Error: 'invalid_model' is not one of ['mistralai/Voxtral-Mini-3B-2507', 'mistralai/Voxtral-Small-24B-2507']\n",
      "Valid port change: Valid=False\n",
      "  Error: 'model_id' is a required property\n",
      "\n",
      "Failed validating 'required' in schema:\n",
      "    {'$schema': 'http://j\n",
      "Temperature out of range: Valid=False\n",
      "  Error: 'model_id' is a required property\n",
      "\n",
      "Failed validating 'required' in schema:\n",
      "    {'$schema': 'http://j\n"
     ]
    }
   ],
   "source": [
    "# Test configuration validation\n",
    "test_configs = [\n",
    "    ({\"model_id\": \"mistralai/Voxtral-Mini-3B-2507\"}, \"Valid config\"),\n",
    "    ({\"model_id\": \"invalid_model\"}, \"Invalid model\"),\n",
    "    ({\"server_port\": 9000}, \"Valid port change\"),\n",
    "    ({\"temperature\": 2.5}, \"Temperature out of range\"),\n",
    "]\n",
    "\n",
    "for config, description in test_configs:\n",
    "    is_valid, error = plugin.validate_config(config)\n",
    "    print(f\"{description}: Valid={is_valid}\")\n",
    "    if error:\n",
    "        print(f\"  Error: {error[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e3879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current config mode: external\n",
      "Current model: mistralai/Voxtral-Mini-3B-2507\n"
     ]
    }
   ],
   "source": [
    "# Test initialization with external server mode\n",
    "plugin.initialize({\n",
    "    \"model_id\": \"mistralai/Voxtral-Mini-3B-2507\",\n",
    "    \"server_mode\": \"external\",\n",
    "    \"server_url\": \"http://localhost:8000\"\n",
    "})\n",
    "print(f\"Current config mode: {plugin.get_current_config()['server_mode']}\")\n",
    "print(f\"Current model: {plugin.get_current_config()['model_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b85b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
