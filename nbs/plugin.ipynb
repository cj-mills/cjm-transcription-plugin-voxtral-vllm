{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e337c1ba",
   "metadata": {},
   "source": [
    "# Voxtral VLLM Plugin\n",
    "\n",
    "> Plugin implementation for Mistral Voxtral transcription through vLLM server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6262b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c18907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from dataclasses import replace as dataclass_replace\n",
    "from typing import Dict, Any, Optional, List, Union, Generator\n",
    "import tempfile\n",
    "import warnings\n",
    "from threading import Thread\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import atexit\n",
    "import signal\n",
    "import threading\n",
    "import queue\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "from fastcore.basics import patch\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    OPENAI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPENAI_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from mistral_common.protocol.transcription.request import TranscriptionRequest\n",
    "    from mistral_common.protocol.instruct.chunk import RawAudio\n",
    "    from mistral_common.audio import Audio\n",
    "    MISTRAL_COMMON_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MISTRAL_COMMON_AVAILABLE = False\n",
    "    \n",
    "from cjm_transcription_plugin_system.plugin_interface import TranscriptionPlugin\n",
    "from cjm_transcription_plugin_system.core import AudioData, TranscriptionResult\n",
    "from cjm_plugin_system.utils.validation import (\n",
    "    dict_to_config, config_to_dict, validate_config,\n",
    "    SCHEMA_TITLE, SCHEMA_DESC, SCHEMA_MIN, SCHEMA_MAX, SCHEMA_ENUM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import atexit\n",
    "import signal\n",
    "import threading\n",
    "import queue\n",
    "import re\n",
    "from contextlib import contextmanager\n",
    "from typing import Optional, Callable, List\n",
    "from datetime import datetime\n",
    "\n",
    "class VLLMServer:\n",
    "    \"\"\"vLLM server manager for Voxtral models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"mistralai/Voxtral-Mini-3B-2507\", # Model name to serve\n",
    "        port: int = 8000, # Port for the server\n",
    "        host: str = \"0.0.0.0\", # Host address to bind to\n",
    "        gpu_memory_utilization: float = 0.85, # Fraction of GPU memory to use\n",
    "        log_level: str = \"INFO\", # Logging level (DEBUG, INFO, WARNING, ERROR)\n",
    "        capture_logs: bool = True, # Whether to capture and display server logs\n",
    "        **kwargs # Additional vLLM server arguments\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.port = port\n",
    "        self.host = host\n",
    "        self.base_url = f\"http://localhost:{self.port}\"\n",
    "        self.process: Optional[subprocess.Popen] = None\n",
    "        self.capture_logs = capture_logs\n",
    "        self.log_level = log_level\n",
    "        \n",
    "        # Log management\n",
    "        self.log_queue = queue.Queue()\n",
    "        self.log_thread = None\n",
    "        self.stop_logging = threading.Event()\n",
    "        self.log_callbacks: List[Callable] = []\n",
    "        \n",
    "        # Build command\n",
    "        self.cmd = [\n",
    "            \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "            \"--model\", model,\n",
    "            \"--port\", str(self.port),\n",
    "            \"--host\", host,\n",
    "            \"--gpu-memory-utilization\", str(gpu_memory_utilization),\n",
    "            \"--tokenizer-mode\", \"mistral\",\n",
    "            \"--config-format\", \"mistral\",\n",
    "            \"--load-format\", \"mistral\",\n",
    "        ]\n",
    "        \n",
    "        # Add any additional arguments\n",
    "        for key, value in kwargs.items():\n",
    "            self.cmd.extend([f\"--{key.replace('_', '-')}\", str(value)])\n",
    "    \n",
    "    def add_log_callback(\n",
    "        self, \n",
    "        callback: Callable[[str], None] # Function that receives log line strings\n",
    "    ) -> None: # Returns nothing\n",
    "        \"\"\"Add a callback function to receive each log line.\"\"\"\n",
    "        self.log_callbacks.append(callback)\n",
    "    \n",
    "    def _process_log_line(\n",
    "        self, \n",
    "        line: str # Log line to process\n",
    "    ) -> None: # Returns nothing\n",
    "        \"\"\"Process a single log line.\"\"\"\n",
    "        if not line.strip():\n",
    "            return\n",
    "        \n",
    "        # Add timestamp if not present\n",
    "        if not re.match(r'^\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', line):\n",
    "            timestamp = datetime.now().strftime(\"%m-%d %H:%M:%S\")\n",
    "            line = f\"{timestamp} {line}\"\n",
    "        \n",
    "        # Store in queue for retrieval\n",
    "        self.log_queue.put(line)\n",
    "        \n",
    "        # Call callbacks\n",
    "        for callback in self.log_callbacks:\n",
    "            try:\n",
    "                callback(line)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in log callback: {e}\")\n",
    "        \n",
    "        # Print if we're capturing logs\n",
    "        if self.capture_logs:\n",
    "            # Color code based on log level\n",
    "            if \"ERROR\" in line:\n",
    "                print(f\"\\033[91m{line}\\033[0m\")  # Red\n",
    "            elif \"WARNING\" in line:\n",
    "                print(f\"\\033[93m{line}\\033[0m\")  # Yellow\n",
    "            elif \"INFO\" in line:\n",
    "                print(f\"\\033[94m{line}\\033[0m\")  # Blue\n",
    "            else:\n",
    "                print(line)\n",
    "    \n",
    "    def _log_reader(\n",
    "        self, \n",
    "        pipe, # Pipe to read from\n",
    "        pipe_name: str # Name of the pipe (stdout/stderr)\n",
    "    ) -> None: # Returns nothing\n",
    "        \"\"\"Read logs from a pipe in a separate thread.\"\"\"\n",
    "        for line in iter(pipe.readline, ''):\n",
    "            if self.stop_logging.is_set():\n",
    "                break\n",
    "            if line:\n",
    "                # Add pipe identifier for debugging\n",
    "                if pipe_name == \"stderr\" and \"INFO\" in line:\n",
    "                    # Most INFO logs come through stderr in vLLM\n",
    "                    self._process_log_line(line.strip())\n",
    "                elif pipe_name == \"stdout\":\n",
    "                    self._process_log_line(line.strip())\n",
    "    \n",
    "    def start(\n",
    "        self, \n",
    "        wait_for_ready: bool = True, # Wait for server to be ready before returning\n",
    "        timeout: int = 120, # Maximum seconds to wait for server readiness\n",
    "        show_progress: bool = True # Show progress indicators during startup\n",
    "    ) -> None: # Returns nothing\n",
    "        \"\"\"Start the vLLM server.\"\"\"\n",
    "        if self.is_running():\n",
    "            print(\"Server is already running\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Starting vLLM server with model {self.model}...\")\n",
    "        \n",
    "        # Start process with pipes for output capture\n",
    "        self.process = subprocess.Popen(\n",
    "            self.cmd,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1,  # Line buffered\n",
    "            universal_newlines=True,\n",
    "            preexec_fn=lambda: signal.signal(signal.SIGINT, signal.SIG_IGN)\n",
    "        )\n",
    "        \n",
    "        # Start log reading threads\n",
    "        self.stop_logging.clear()\n",
    "        \n",
    "        if self.capture_logs:\n",
    "            # Create threads for reading stdout and stderr\n",
    "            stdout_thread = threading.Thread(\n",
    "                target=self._log_reader,\n",
    "                args=(self.process.stdout, \"stdout\"),\n",
    "                daemon=True\n",
    "            )\n",
    "            stderr_thread = threading.Thread(\n",
    "                target=self._log_reader,\n",
    "                args=(self.process.stderr, \"stderr\"),\n",
    "                daemon=True\n",
    "            )\n",
    "            stdout_thread.start()\n",
    "            stderr_thread.start()\n",
    "        \n",
    "        # Register cleanup on exit\n",
    "        atexit.register(self.stop)\n",
    "        \n",
    "        if wait_for_ready:\n",
    "            self._wait_for_server(timeout, show_progress)\n",
    "    \n",
    "    def _wait_for_server(\n",
    "        self, \n",
    "        timeout: int, # Maximum seconds to wait\n",
    "        show_progress: bool # Whether to show progress indicators\n",
    "    ) -> None: # Returns nothing\n",
    "        \"\"\"Wait for server to be ready to accept requests.\"\"\"\n",
    "        start_time = time.time()\n",
    "        last_status = \"\"\n",
    "        \n",
    "        # Key phases to look for in logs\n",
    "        phases = {\n",
    "            \"detected platform\": \"üîç Detecting platform...\",\n",
    "            \"Loading model\": \"üì¶ Loading model weights...\",\n",
    "            \"downloading weights\": \"‚¨áÔ∏è Downloading model weights...\",\n",
    "            \"Loading weights took\": \"‚úÖ Model weights loaded\",\n",
    "            \"Graph capturing\": \"üìä Capturing CUDA graphs...\",\n",
    "            \"Graph capturing finished\": \"‚úÖ CUDA graphs ready\",\n",
    "            \"Starting vLLM API server\": \"üöÄ Starting API server...\",\n",
    "            \"Available routes\": \"‚úÖ Server ready!\"\n",
    "        }\n",
    "        \n",
    "        while time.time() - start_time < timeout:\n",
    "            # Check for new status in logs\n",
    "            if show_progress and not self.log_queue.empty():\n",
    "                try:\n",
    "                    log_line = self.log_queue.get_nowait()\n",
    "                    for key, status_msg in phases.items():\n",
    "                        if key in log_line:\n",
    "                            if status_msg != last_status:\n",
    "                                print(f\"\\n  {status_msg}\")\n",
    "                                last_status = status_msg\n",
    "                            break\n",
    "                except queue.Empty:\n",
    "                    pass\n",
    "            \n",
    "            # Try to connect to the server\n",
    "            try:\n",
    "                response = requests.get(f\"{self.base_url}/health\", timeout=1)\n",
    "                if response.status_code == 200:\n",
    "                    print(f\"\\n‚úÖ vLLM server is ready at {self.base_url}\")\n",
    "                    return\n",
    "            except requests.exceptions.RequestException:\n",
    "                pass\n",
    "            \n",
    "            # Check if process has crashed\n",
    "            if self.process.poll() is not None:\n",
    "                raise RuntimeError(f\"Server process exited with code {self.process.poll()}\")\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        raise TimeoutError(f\"Server did not start within {timeout} seconds\")\n",
    "    \n",
    "    def stop(self) -> None: # Returns nothing\n",
    "        \"\"\"Stop the vLLM server.\"\"\"\n",
    "        if self.process and self.process.poll() is None:\n",
    "            print(\"Stopping vLLM server...\")\n",
    "            \n",
    "            # Signal threads to stop\n",
    "            self.stop_logging.set()\n",
    "            \n",
    "            # Terminate process\n",
    "            self.process.terminate()\n",
    "            try:\n",
    "                self.process.wait(timeout=10)\n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(\"Force killing server...\")\n",
    "                self.process.kill()\n",
    "                self.process.wait()\n",
    "            \n",
    "            self.process = None\n",
    "            print(\"Server stopped\")\n",
    "    \n",
    "    def restart(self) -> None: # Returns nothing\n",
    "        \"\"\"Restart the server.\"\"\"\n",
    "        self.stop()\n",
    "        time.sleep(2)\n",
    "        self.start()\n",
    "    \n",
    "    def is_running(self) -> bool: # True if server is running and responsive\n",
    "        \"\"\"Check if server is running and responsive.\"\"\"\n",
    "        # First check if process exists and hasn't exited\n",
    "        if self.process is None or self.process.poll() is not None:\n",
    "            return False\n",
    "        \n",
    "        # Try to connect to the server\n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}/health\", timeout=1)\n",
    "            if response.status_code == 200:\n",
    "                # print(f\"\\n‚úÖ vLLM server is ready at {self.base_url}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"\\n‚ùå vLLM server is not ready at {self.base_url}\")\n",
    "                return False\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\n‚ùå The following exception occurred: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_recent_logs(\n",
    "        self, \n",
    "        n: int = 100 # Number of recent log lines to retrieve\n",
    "    ) -> List[str]: # List of recent log lines\n",
    "        \"\"\"Get the most recent n log lines.\"\"\"\n",
    "        logs = []\n",
    "        while not self.log_queue.empty() and len(logs) < n:\n",
    "            try:\n",
    "                logs.append(self.log_queue.get_nowait())\n",
    "            except queue.Empty:\n",
    "                break\n",
    "        return logs\n",
    "    \n",
    "    def get_metrics_from_logs(self) -> dict: # Dictionary with performance metrics\n",
    "        \"\"\"Parse recent logs to extract performance metrics.\"\"\"\n",
    "        metrics = {\n",
    "            \"prompt_throughput\": 0.0,\n",
    "            \"generation_throughput\": 0.0,\n",
    "            \"running_requests\": 0,\n",
    "            \"waiting_requests\": 0,\n",
    "            \"gpu_kv_cache_usage\": 0.0,\n",
    "            \"prefix_cache_hit_rate\": 0.0,\n",
    "        }\n",
    "        \n",
    "        # Look for metric lines in recent logs\n",
    "        recent_logs = self.get_recent_logs(50)\n",
    "        for log in recent_logs:\n",
    "            if \"Avg prompt throughput:\" in log:\n",
    "                # Parse metrics line\n",
    "                match = re.search(r'Avg prompt throughput: ([\\d.]+) tokens/s', log)\n",
    "                if match:\n",
    "                    metrics[\"prompt_throughput\"] = float(match.group(1))\n",
    "                \n",
    "                match = re.search(r'Avg generation throughput: ([\\d.]+) tokens/s', log)\n",
    "                if match:\n",
    "                    metrics[\"generation_throughput\"] = float(match.group(1))\n",
    "                \n",
    "                match = re.search(r'Running: (\\d+) reqs', log)\n",
    "                if match:\n",
    "                    metrics[\"running_requests\"] = int(match.group(1))\n",
    "                \n",
    "                match = re.search(r'Waiting: (\\d+) reqs', log)\n",
    "                if match:\n",
    "                    metrics[\"waiting_requests\"] = int(match.group(1))\n",
    "                \n",
    "                match = re.search(r'GPU KV cache usage: ([\\d.]+)%', log)\n",
    "                if match:\n",
    "                    metrics[\"gpu_kv_cache_usage\"] = float(match.group(1))\n",
    "                \n",
    "                match = re.search(r'Prefix cache hit rate: ([\\d.]+)%', log)\n",
    "                if match:\n",
    "                    metrics[\"prefix_cache_hit_rate\"] = float(match.group(1))\n",
    "                \n",
    "                break  # Use the most recent metrics\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def tail_logs(\n",
    "        self, \n",
    "        follow: bool = True, # Continue displaying new logs as they arrive\n",
    "        n: int = 10 # Number of initial lines to display\n",
    "    ) -> None: # Returns nothing\n",
    "        \"\"\"Tail the server logs (similar to tail -f).\"\"\"\n",
    "        # Display recent logs\n",
    "        recent = self.get_recent_logs(n)\n",
    "        for line in recent:\n",
    "            print(line)\n",
    "        \n",
    "        if follow:\n",
    "            print(\"\\n--- Following logs (Ctrl+C to stop) ---\\n\")\n",
    "            try:\n",
    "                while self.is_running():\n",
    "                    if not self.log_queue.empty():\n",
    "                        print(self.log_queue.get())\n",
    "                    else:\n",
    "                        time.sleep(0.1)\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n--- Stopped following logs ---\")\n",
    "    \n",
    "    def __enter__(self):\n",
    "        \"\"\"Context manager entry.\"\"\"\n",
    "        self.start()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Context manager exit.\"\"\"\n",
    "        self.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff4bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class VoxtralVLLMPluginConfig:\n",
    "    \"\"\"Configuration for Voxtral VLLM transcription plugin.\"\"\"\n",
    "    model_id:str = field(\n",
    "        default=\"mistralai/Voxtral-Mini-3B-2507\",\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Model ID\",\n",
    "            SCHEMA_DESC: \"Voxtral model to use. Mini is faster, Small is more accurate.\",\n",
    "            SCHEMA_ENUM: [\"mistralai/Voxtral-Mini-3B-2507\", \"mistralai/Voxtral-Small-24B-2507\"]\n",
    "        }\n",
    "    )\n",
    "    server_mode:str = field(\n",
    "        default=\"managed\",\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Server Mode\",\n",
    "            SCHEMA_DESC: \"'managed': plugin manages server lifecycle, 'external': connect to existing server\",\n",
    "            SCHEMA_ENUM: [\"managed\", \"external\"]\n",
    "        }\n",
    "    )\n",
    "    server_url:str = field(\n",
    "        default=\"http://localhost:8000\",\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Server URL\",\n",
    "            SCHEMA_DESC: \"vLLM server URL (for external mode)\"\n",
    "        }\n",
    "    )\n",
    "    server_port:int = field(\n",
    "        default=8000,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Server Port\",\n",
    "            SCHEMA_DESC: \"Port for managed vLLM server\",\n",
    "            SCHEMA_MIN: 1024,\n",
    "            SCHEMA_MAX: 65535\n",
    "        }\n",
    "    )\n",
    "    gpu_memory_utilization:float = field(\n",
    "        default=0.85,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"GPU Memory Utilization\",\n",
    "            SCHEMA_DESC: \"Fraction of GPU memory to use (managed mode)\",\n",
    "            SCHEMA_MIN: 0.1,\n",
    "            SCHEMA_MAX: 1.0\n",
    "        }\n",
    "    )\n",
    "    max_model_len:int = field(\n",
    "        default=32768,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Max Model Length\",\n",
    "            SCHEMA_DESC: \"Maximum sequence length for the model\",\n",
    "            SCHEMA_MIN: 1024,\n",
    "            SCHEMA_MAX: 131072\n",
    "        }\n",
    "    )\n",
    "    language:Optional[str] = field(\n",
    "        default=\"en\",\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Language\",\n",
    "            SCHEMA_DESC: \"Language code for transcription (e.g., 'en', 'es', 'fr')\"\n",
    "        }\n",
    "    )\n",
    "    temperature:float = field(\n",
    "        default=0.0,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Temperature\",\n",
    "            SCHEMA_DESC: \"Temperature for sampling (0.0 for deterministic)\",\n",
    "            SCHEMA_MIN: 0.0,\n",
    "            SCHEMA_MAX: 2.0\n",
    "        }\n",
    "    )\n",
    "    streaming:bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Streaming\",\n",
    "            SCHEMA_DESC: \"Enable streaming output by default\"\n",
    "        }\n",
    "    )\n",
    "    server_startup_timeout:int = field(\n",
    "        default=120,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Server Startup Timeout\",\n",
    "            SCHEMA_DESC: \"Timeout in seconds for server startup (managed mode)\",\n",
    "            SCHEMA_MIN: 30,\n",
    "            SCHEMA_MAX: 600\n",
    "        }\n",
    "    )\n",
    "    auto_start_server:bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Auto Start Server\",\n",
    "            SCHEMA_DESC: \"Automatically start server on first use (managed mode)\"\n",
    "        }\n",
    "    )\n",
    "    capture_server_logs:bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Capture Server Logs\",\n",
    "            SCHEMA_DESC: \"Capture vLLM server logs (managed mode)\"\n",
    "        }\n",
    "    )\n",
    "    dtype:str = field(\n",
    "        default=\"auto\",\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Data Type\",\n",
    "            SCHEMA_DESC: \"Data type for model weights\",\n",
    "            SCHEMA_ENUM: [\"auto\", \"half\", \"float16\", \"bfloat16\", \"float32\"]\n",
    "        }\n",
    "    )\n",
    "    tensor_parallel_size:int = field(\n",
    "        default=1,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Tensor Parallel Size\",\n",
    "            SCHEMA_DESC: \"Number of GPUs for tensor parallelism\",\n",
    "            SCHEMA_MIN: 1,\n",
    "            SCHEMA_MAX: 8\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "class VoxtralVLLMPlugin(TranscriptionPlugin):\n",
    "    \"\"\"Mistral Voxtral transcription plugin via vLLM server.\"\"\"\n",
    "    \n",
    "    config_class = VoxtralVLLMPluginConfig\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the Voxtral VLLM plugin with default configuration.\"\"\"\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{type(self).__name__}\")\n",
    "        self.config: VoxtralVLLMPluginConfig = None\n",
    "        self.server: Optional[VLLMServer] = None\n",
    "        self.client: Optional[OpenAI] = None\n",
    "        self.model_id: Optional[str] = None\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str: # The plugin name identifier\n",
    "        \"\"\"Get the plugin name identifier.\"\"\"\n",
    "        return \"voxtral_vllm\"\n",
    "    \n",
    "    @property\n",
    "    def version(self) -> str: # The plugin version string\n",
    "        \"\"\"Get the plugin version string.\"\"\"\n",
    "        return \"1.0.0\"\n",
    "    \n",
    "    @property\n",
    "    def supported_formats(self) -> List[str]: # List of supported audio formats\n",
    "        \"\"\"Get the list of supported audio file formats.\"\"\"\n",
    "        return [\"wav\", \"mp3\", \"flac\", \"m4a\", \"ogg\", \"webm\", \"mp4\", \"avi\", \"mov\"]\n",
    "    \n",
    "    def get_current_config(self) -> VoxtralVLLMPluginConfig: # Current configuration dataclass\n",
    "        \"\"\"Return current configuration.\"\"\"\n",
    "        return self.config\n",
    "    \n",
    "    def initialize(\n",
    "        self,\n",
    "        config: Optional[Any] = None # Configuration dataclass, dict, or None\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the plugin with configuration.\"\"\"\n",
    "        # Handle config input\n",
    "        if config is None:\n",
    "            self.config = VoxtralVLLMPluginConfig()\n",
    "        elif isinstance(config, VoxtralVLLMPluginConfig):\n",
    "            self.config = config\n",
    "        elif isinstance(config, dict):\n",
    "            self.config = dict_to_config(VoxtralVLLMPluginConfig, config, validate=True)\n",
    "        else:\n",
    "            raise TypeError(f\"Expected VoxtralVLLMPluginConfig, dict, or None, got {type(config).__name__}\")\n",
    "        \n",
    "        self.model_id = self.config.model_id\n",
    "        \n",
    "        # Initialize based on server mode\n",
    "        if self.config.server_mode == \"managed\":\n",
    "            # Create managed server instance (but don't start yet)\n",
    "            self.server = VLLMServer(\n",
    "                model=self.model_id,\n",
    "                port=self.config.server_port,\n",
    "                gpu_memory_utilization=self.config.gpu_memory_utilization,\n",
    "                max_model_len=self.config.max_model_len,\n",
    "                capture_logs=self.config.capture_server_logs,\n",
    "                dtype=self.config.dtype,\n",
    "                tensor_parallel_size=self.config.tensor_parallel_size\n",
    "            )\n",
    "            server_url = f\"http://localhost:{self.config.server_port}\"\n",
    "        else:\n",
    "            # External server mode\n",
    "            server_url = self.config.server_url\n",
    "        \n",
    "        # Create OpenAI client\n",
    "        self.client = OpenAI(\n",
    "            api_key=\"EMPTY\",  # vLLM doesn't require API key\n",
    "            base_url=f\"{server_url}/v1\"\n",
    "        )\n",
    "        \n",
    "        self.logger.info(\n",
    "            f\"Initialized Voxtral VLLM plugin with model '{self.model_id}' \"\n",
    "            f\"in {self.config.server_mode} mode\"\n",
    "        )\n",
    "    \n",
    "    def _ensure_server_running(self) -> None:\n",
    "        \"\"\"Ensure the vLLM server is running (for managed mode).\"\"\"\n",
    "        if self.config.server_mode == \"managed\" and self.server:\n",
    "            if not self.server.is_running():\n",
    "                print(\"\\n\\nSERVER IS NOT RUNNING\\n\\n\")\n",
    "                if self.config.auto_start_server:\n",
    "                    self.logger.info(\"Starting vLLM server...\")\n",
    "                    self.server.start(\n",
    "                        wait_for_ready=True,\n",
    "                        timeout=self.config.server_startup_timeout,\n",
    "                        show_progress=self.config.capture_server_logs\n",
    "                    )\n",
    "                else:\n",
    "                    raise RuntimeError(\"vLLM server is not running and auto_start_server is disabled\")\n",
    "        elif self.config.server_mode == \"external\":\n",
    "            # Check if external server is accessible\n",
    "            try:\n",
    "                response = requests.get(f\"{self.config.server_url}/health\", timeout=5)\n",
    "                if response.status_code != 200:\n",
    "                    raise RuntimeError(f\"External vLLM server returned status {response.status_code}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                raise RuntimeError(f\"Cannot connect to external vLLM server: {e}\")\n",
    "    \n",
    "    def _prepare_audio(\n",
    "        self,\n",
    "        audio: Union[AudioData, str, Path] # Audio data, file path, or Path object to prepare\n",
    "    ) -> str: # Path to the prepared audio file\n",
    "        \"\"\"Prepare audio for Voxtral processing.\"\"\"\n",
    "        if isinstance(audio, (str, Path)):\n",
    "            # Already a file path\n",
    "            return str(audio)\n",
    "        \n",
    "        elif isinstance(audio, AudioData):\n",
    "            # Save AudioData to temporary file\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_file:\n",
    "                # Ensure audio is in the correct format\n",
    "                audio_array = audio.samples\n",
    "                \n",
    "                # If stereo, convert to mono\n",
    "                if audio_array.ndim > 1:\n",
    "                    audio_array = audio_array.mean(axis=1)\n",
    "                \n",
    "                # Ensure float32 and normalized\n",
    "                if audio_array.dtype != np.float32:\n",
    "                    audio_array = audio_array.astype(np.float32)\n",
    "                \n",
    "                # Normalize if needed\n",
    "                if audio_array.max() > 1.0:\n",
    "                    audio_array = audio_array / np.abs(audio_array).max()\n",
    "                \n",
    "                # Save to file\n",
    "                sf.write(tmp_file.name, audio_array, audio.sample_rate)\n",
    "                return tmp_file.name\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported audio input type: {type(audio)}\")\n",
    "    \n",
    "    def execute(\n",
    "        self,\n",
    "        audio: Union[AudioData, str, Path], # Audio data or path to audio file to transcribe\n",
    "        **kwargs # Additional arguments to override config\n",
    "    ) -> TranscriptionResult: # Transcription result with text and metadata\n",
    "        \"\"\"Transcribe audio using Voxtral via vLLM.\"\"\"\n",
    "        # Ensure server is running\n",
    "        self._ensure_server_running()\n",
    "        \n",
    "        # Prepare audio file\n",
    "        audio_path = self._prepare_audio(audio)\n",
    "        temp_file_created = not isinstance(audio, (str, Path))\n",
    "        \n",
    "        try:\n",
    "            # Get config values, allowing kwargs overrides\n",
    "            language = kwargs.get(\"language\", self.config.language)\n",
    "            temperature = kwargs.get(\"temperature\", self.config.temperature)\n",
    "            \n",
    "            # Prepare inputs using mistral_common\n",
    "            self.logger.info(f\"Processing audio with Voxtral {self.model_id} via vLLM\")\n",
    "            \n",
    "            input_audio = Audio.from_file(audio_path, strict=False)\n",
    "            input_audio = RawAudio.from_audio(input_audio)\n",
    "            \n",
    "            req = TranscriptionRequest(\n",
    "                model=self.model_id,\n",
    "                audio=input_audio,\n",
    "                language=language or \"en\",\n",
    "                temperature=temperature\n",
    "            ).to_openai(exclude=(\"top_p\", \"seed\"))\n",
    "            \n",
    "            # Get transcription from vLLM server\n",
    "            response = self.client.audio.transcriptions.create(**req)\n",
    "            \n",
    "            # Create transcription result\n",
    "            transcription_result = TranscriptionResult(\n",
    "                text=response.text.strip(),\n",
    "                confidence=None,  # vLLM doesn't provide confidence scores\n",
    "                segments=None,  # vLLM doesn't provide segments by default\n",
    "                metadata={\n",
    "                    \"model\": self.model_id,\n",
    "                    \"language\": language or \"en\",\n",
    "                    \"server_mode\": self.config.server_mode,\n",
    "                    \"temperature\": temperature,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f\"Transcription completed: {len(response.text.split())} words\")\n",
    "            return transcription_result\n",
    "            \n",
    "        finally:\n",
    "            # Clean up temporary file if created\n",
    "            if temp_file_created:\n",
    "                try:\n",
    "                    Path(audio_path).unlink()\n",
    "                except Exception:\n",
    "                    pass\n",
    "    \n",
    "    def is_available(self) -> bool: # True if vLLM and dependencies are available\n",
    "        \"\"\"Check if vLLM and required dependencies are available.\"\"\"\n",
    "        if not OPENAI_AVAILABLE:\n",
    "            return False\n",
    "        if not MISTRAL_COMMON_AVAILABLE:\n",
    "            return False\n",
    "        \n",
    "        # Check if vLLM is installed\n",
    "        try:\n",
    "            import vllm\n",
    "            return True\n",
    "        except ImportError:\n",
    "            return False\n",
    "    \n",
    "    def cleanup(self) -> None:\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        self.logger.info(\"Cleaning up Voxtral VLLM plugin\")\n",
    "        \n",
    "        # Stop managed server if running\n",
    "        if self.config and self.config.server_mode == \"managed\" and self.server:\n",
    "            if self.server.is_running():\n",
    "                self.logger.info(\"Stopping managed vLLM server\")\n",
    "                self.server.stop()\n",
    "            self.server = None\n",
    "        \n",
    "        # Clear client\n",
    "        self.client = None\n",
    "        \n",
    "        self.logger.info(\"Cleanup completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef2767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def supports_streaming(\n",
    "    self: VoxtralVLLMPlugin # The plugin instance\n",
    ") -> bool: # True if streaming is supported\n",
    "    \"\"\"Check if this plugin supports streaming transcription.\"\"\"\n",
    "    return True\n",
    "\n",
    "@patch\n",
    "def execute_stream(\n",
    "    self: VoxtralVLLMPlugin, # The plugin instance\n",
    "    audio: Union[AudioData, str, Path], # Audio data or path to audio file\n",
    "    **kwargs # Additional plugin-specific parameters\n",
    ") -> Generator[str, None, TranscriptionResult]: # Yields text chunks, returns final result\n",
    "    \"\"\"Stream transcription results chunk by chunk.\"\"\"\n",
    "    # Ensure server is running\n",
    "    self._ensure_server_running()\n",
    "    \n",
    "    # Prepare audio file\n",
    "    audio_path = self._prepare_audio(audio)\n",
    "    temp_file_created = not isinstance(audio, (str, Path))\n",
    "    \n",
    "    try:\n",
    "        # Get config values, allowing kwargs overrides\n",
    "        language = kwargs.get(\"language\", self.config.language)\n",
    "        temperature = kwargs.get(\"temperature\", self.config.temperature)\n",
    "        \n",
    "        # Prepare inputs using mistral_common\n",
    "        self.logger.info(f\"Streaming transcription with Voxtral {self.model_id} via vLLM\")\n",
    "        \n",
    "        input_audio = Audio.from_file(audio_path, strict=False)\n",
    "        input_audio = RawAudio.from_audio(input_audio)\n",
    "        \n",
    "        req = TranscriptionRequest(\n",
    "            model=self.model_id,\n",
    "            audio=input_audio,\n",
    "            language=language or \"en\",\n",
    "            temperature=temperature\n",
    "        ).to_openai(exclude=(\"top_p\", \"seed\"))\n",
    "        \n",
    "        # Get streaming transcription from vLLM server\n",
    "        response = self.client.audio.transcriptions.create(**req, stream=True)\n",
    "        \n",
    "        # Collect generated text\n",
    "        generated_text = \"\"\n",
    "        for chunk in response:\n",
    "            if chunk.choices[0]['delta']['content']:\n",
    "                text_chunk = chunk.choices[0]['delta']['content']\n",
    "                generated_text += text_chunk\n",
    "                yield text_chunk\n",
    "        \n",
    "        # Return final result\n",
    "        return TranscriptionResult(\n",
    "            text=generated_text.strip(),\n",
    "            confidence=None,\n",
    "            segments=None,\n",
    "            metadata={\n",
    "                \"model\": self.model_id,\n",
    "                \"language\": language or \"en\",\n",
    "                \"server_mode\": self.config.server_mode,\n",
    "                \"temperature\": temperature,\n",
    "                \"streaming\": True,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    finally:\n",
    "        # Clean up temporary file if created\n",
    "        if temp_file_created:\n",
    "            try:\n",
    "                Path(audio_path).unlink()\n",
    "            except Exception:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec7ebca",
   "metadata": {},
   "source": [
    "## Testing the Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbbf470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voxtral VLLM available: True\n",
      "Plugin name: voxtral_vllm\n",
      "Plugin version: 1.0.0\n",
      "Supported formats: ['wav', 'mp3', 'flac', 'm4a', 'ogg', 'webm', 'mp4', 'avi', 'mov']\n",
      "Supports streaming: True\n"
     ]
    }
   ],
   "source": [
    "# Test basic functionality\n",
    "plugin = VoxtralVLLMPlugin()\n",
    "\n",
    "# Check availability\n",
    "print(f\"Voxtral VLLM available: {plugin.is_available()}\")\n",
    "print(f\"Plugin name: {plugin.name}\")\n",
    "print(f\"Plugin version: {plugin.version}\")\n",
    "print(f\"Supported formats: {plugin.supported_formats}\")\n",
    "print(f\"Supports streaming: {plugin.supports_streaming()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb1519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "  - mistralai/Voxtral-Mini-3B-2507\n",
      "  - mistralai/Voxtral-Small-24B-2507\n",
      "\n",
      "Server modes: ['managed', 'external']\n"
     ]
    }
   ],
   "source": [
    "# Test configuration dataclass\n",
    "from dataclasses import fields\n",
    "\n",
    "print(\"Available models:\")\n",
    "model_field = next(f for f in fields(VoxtralVLLMPluginConfig) if f.name == \"model_id\")\n",
    "for model in model_field.metadata.get(SCHEMA_ENUM, []):\n",
    "    print(f\"  - {model}\")\n",
    "\n",
    "server_field = next(f for f in fields(VoxtralVLLMPluginConfig) if f.name == \"server_mode\")\n",
    "print(f\"\\nServer modes: {server_field.metadata.get(SCHEMA_ENUM)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd4482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid config: Valid=True\n",
      "Invalid model: Valid=False\n",
      "  Error: model_id: 'invalid_model' is not one of ['mistralai/Voxtral-Mini-3B-2507', 'mistralai/Voxtral-Small-\n",
      "Valid port change: Valid=True\n",
      "Temperature out of range: Valid=False\n",
      "  Error: temperature: 2.5 is greater than maximum 2.0\n"
     ]
    }
   ],
   "source": [
    "# Test configuration validation\n",
    "from dataclasses import asdict\n",
    "\n",
    "plugin = VoxtralVLLMPlugin()\n",
    "\n",
    "test_configs = [\n",
    "    ({\"model_id\": \"mistralai/Voxtral-Mini-3B-2507\"}, \"Valid config\"),\n",
    "    ({\"model_id\": \"invalid_model\"}, \"Invalid model\"),\n",
    "    ({\"server_port\": 9000}, \"Valid port change\"),\n",
    "    ({\"temperature\": 2.5}, \"Temperature out of range\"),\n",
    "]\n",
    "\n",
    "# Get defaults for merging\n",
    "defaults = plugin.get_config_defaults()\n",
    "\n",
    "for config_update, description in test_configs:\n",
    "    try:\n",
    "        merged = {**defaults, **config_update}\n",
    "        test_config = dict_to_config(VoxtralVLLMPluginConfig, merged, validate=True)\n",
    "        print(f\"{description}: Valid=True\")\n",
    "    except ValueError as e:\n",
    "        print(f\"{description}: Valid=False\")\n",
    "        print(f\"  Error: {str(e)[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e3879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current config mode: external\n",
      "Current model: mistralai/Voxtral-Mini-3B-2507\n"
     ]
    }
   ],
   "source": [
    "# Test initialization with external server mode\n",
    "plugin.initialize({\n",
    "    \"model_id\": \"mistralai/Voxtral-Mini-3B-2507\",\n",
    "    \"server_mode\": \"external\",\n",
    "    \"server_url\": \"http://localhost:8000\"\n",
    "})\n",
    "print(f\"Current config mode: {plugin.get_current_config().server_mode}\")\n",
    "print(f\"Current model: {plugin.get_current_config().model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b85b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
